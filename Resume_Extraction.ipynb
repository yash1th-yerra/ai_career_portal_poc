{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "777545bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import docx2txt\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import urllib.parse\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Any\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import google.generativeai as genai\n",
    "import spacy\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49c9c3b",
   "metadata": {},
   "source": [
    "### Approach 1 – Raw text → LLM\n",
    "\n",
    "- Extract text using PyMuPDF/docx2txt/Tesseract\n",
    "\n",
    "- Call LLM to structure into JSON\n",
    "\n",
    "- Log accuracy, tokens, latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee28bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0abe2d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_file(file_path: str, file_type: str) -> str:\n",
    "    \"\"\"Extract text from PDF or DOCX resume.\"\"\"\n",
    "    if file_type.lower() == \"pdf\":\n",
    "        text_parts = []\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            max_pages = min(2, len(pdf.pages))\n",
    "            for i in range(max_pages):\n",
    "                text_parts.append(pdf.pages[i].extract_text() or \"\")\n",
    "        text = \"\\n\".join(text_parts)\n",
    "    elif file_type.lower() == \"docx\":\n",
    "        text = docx2txt.process(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type\")\n",
    "\n",
    "    # Optional cleaning\n",
    "    text = text.replace('\\n', ' ').replace('\\t', ' ').strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a2406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_resume_with_gemini(resume_text):\n",
    "#     prompt = f\"\"\"\n",
    "# You are an expert Resume Intelligence Agent specialized in extracting structured data and evaluating resumes for ATS compatibility.\n",
    "\n",
    "# Analyze the following resume text and return ONLY a valid JSON object with these exact keys:\n",
    "\n",
    "# {{\n",
    "#   \"name\": \"\",\n",
    "#   \"first_name\": \"\",\n",
    "#   \"last_name\": \"\",\n",
    "#   \"email_address\": \"\",\n",
    "#   \"mobile_number\": \"\",\n",
    "#   \"country_code\": \"\",\n",
    "#   \"location\": \"\",\n",
    "#   \"city\": \"\",\n",
    "#   \"state\": \"\",\n",
    "#   \"country\": \"\",\n",
    "#   \"zip_postal_code\": \"\",\n",
    "#   \"summary\": \"\",\n",
    "#   \"skills\": [],\n",
    "#   \"extra_skills\": [],\n",
    "#   \"work_experience\": [],\n",
    "#   \"projects\": [],\n",
    "#   \"certifications\": [],\n",
    "#   \"education\": [],\n",
    "#   \"experience_level\": \"\",\n",
    "#   \"experience_level_number\": 0,\n",
    "#   \"role_keywords\": [],\n",
    "#   \"tech_stack_summary\": \"\",\n",
    "#   \"current_employer\": \"\",\n",
    "#   \"experience_by_skill\": {{}},\n",
    "#   \"availability_questions\": {{\n",
    "#     \"own_laptop_for_evaluation\": \"Yes\",\n",
    "#     \"willing_for_2_3_month_evaluation\": \"Yes\",\n",
    "#     \"available_for_fulltime_internship\": \"Yes\",\n",
    "#     \"preferred_work_timings\": \"Flexible\",\n",
    "#     \"can_start_immediately\": \"Yes\",\n",
    "#     \"work_authorization\": \"Yes\"\n",
    "#   }}\n",
    "# }}\n",
    "\n",
    "# CRITICAL EXTRACTION RULES FOR ALL SECTIONS:\n",
    "\n",
    "# 1. **NAME / FIRST_NAME / LAST_NAME**:  \n",
    "#    - Extract full name exactly as written (usually top of resume).  \n",
    "#    - Split logically into first_name and last_name; if single name, leave last_name empty.\n",
    "\n",
    "# 2. **EMAIL_ADDRESS / MOBILE_NUMBER / COUNTRY_CODE**:  \n",
    "#    - Extract directly from text.  \n",
    "#    - Derive `country_code` if phone number includes it (e.g., \"+91\"), otherwise leave empty.\n",
    "\n",
    "# 3. **LOCATION / CITY / STATE / COUNTRY / ZIP_POSTAL_CODE**:  \n",
    "#    - Extract location only from the contact section.  \n",
    "#    - Derive state/country if missing, e.g., \"Bangalore\" → \"Karnataka, India\".  \n",
    "#    - If city is present but ZIP/postal code is missing, **infer it automatically using known mapping for that city** (e.g., Bangalore → \"560001\").  \n",
    "#    - If city is absent, leave ZIP/postal code empty.  \n",
    "#    - Never use locations mentioned under work experience or projects.\n",
    "\n",
    "# 4. **SUMMARY**:  \n",
    "#    - Extract text from “Summary”, “Profile”, “Objective”, or “About Me” sections.\n",
    "\n",
    "# 5. **SKILLS / EXTRA_SKILLS**:  \n",
    "#    - `skills`: primary professional and technical competencies (normalize capitalization).  \n",
    "#    - `extra_skills`: secondary, contextual, or soft skills (avoid duplicates).\n",
    "\n",
    "# 6. **WORK_EXPERIENCE**:  \n",
    "#    - Each record must include title, company, start_date, end_date (or \"Present\"), duration, and key achievements.  \n",
    "#    - **Set `current_employer` only if the role’s end_date is marked as “Present” or “Ongoing”**.  \n",
    "#    - If all roles are completed (no ongoing job), `current_employer` must be an empty string.\n",
    "\n",
    "# 7. **PROJECTS**:  \n",
    "#    - Each must include name, duration, tech_stack, and short description.  \n",
    "#    - Prefer projects from “Projects” or “Academic Work” sections.\n",
    "\n",
    "# 8. **CERTIFICATIONS**:  \n",
    "#    - Include certification/course name, issuing body, and year if present.\n",
    "\n",
    "# 9. **EDUCATION**:  \n",
    "#    - Include degree, institution, field, and graduation year.  \n",
    "#    - If currently studying, mark as “Pursuing” or “Ongoing”.\n",
    "\n",
    "# 10. **EXPERIENCE_LEVEL**:  \n",
    "#    - Based on total experience:  \n",
    "#      - Internship → 0 years or studying  \n",
    "#      - Entry Level → 0–1 years  \n",
    "#      - Associate → 1–3 years  \n",
    "#      - Mid-Senior Level → 3–7 years  \n",
    "#      - Director → 7+ years  \n",
    "\n",
    "# 11. **EXPERIENCE_LEVEL_NUMBER**:  \n",
    "#    - Internship → 1  \n",
    "#    - Entry Level → 2  \n",
    "#    - Associate → 3  \n",
    "#    - Mid-Senior Level → 4  \n",
    "#    - Director → 5  \n",
    "\n",
    "# 12. **ROLE_KEYWORDS**:  \n",
    "#    - Extract key professional or domain-related keywords (e.g., “Full Stack Development”, “MLOps”, “Data Engineering”, etc.)\n",
    "\n",
    "# 13. **EXPERIENCE_BY_SKILL**:  \n",
    "#    - Map each detected skill to an estimated **integer number of years** based on resume content.  \n",
    "#    - Round intelligently:  \n",
    "#      - < 1 year → 1  \n",
    "#      - 1–1.5 years → 1  \n",
    "#      - 1.6–2.4 years → 2  \n",
    "#      - and so on (round to nearest integer).  \n",
    "#    - Example:  \n",
    "#      \"experience_by_skill\": {{\"Python\": 2, \"React\": 1, \"Spring Boot\": 2}}  \n",
    "#    - Output must use **numeric values only**, without “months” or “years” suffix.\n",
    "\n",
    "# 14. **TECH_STACK_SUMMARY**:  \n",
    "#    - Combine all technical tools, libraries, and frameworks (from skills + projects + experience) into a concise, comma-separated list.\n",
    "\n",
    "# 15. **INFERENCE RULES**:\n",
    "#    - Normalize capitalization for consistency.  \n",
    "#    - Avoid duplicate entries across fields.  \n",
    "#    - Do not fabricate data.  \n",
    "#    - Infer logically only where reasonable (like ZIP from city).\n",
    "\n",
    "# Return ONLY the JSON object, with no explanations, markdown, or text.\n",
    "\n",
    "# Resume Text:\n",
    "# {resume_text}\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "#     start = time.time()\n",
    "#     response = model.generate_content(prompt)\n",
    "#     latency = time.time() - start\n",
    "\n",
    "#     # --- Clean and Parse Gemini Output ---\n",
    "#     raw_output = response.text.strip()\n",
    "\n",
    "#     # Remove code block wrappers if present\n",
    "#     raw_output = re.sub(r\"^```(json)?\", \"\", raw_output)\n",
    "#     raw_output = re.sub(r\"```$\", \"\", raw_output)\n",
    "#     raw_output = raw_output.strip()\n",
    "\n",
    "#     # Try parsing clean JSON\n",
    "#     try:\n",
    "#         structured = json.loads(raw_output)\n",
    "#     except json.JSONDecodeError:\n",
    "#         structured = {\"raw_output\": raw_output}\n",
    "\n",
    "#     return structured, latency, len(prompt.split()), len(response.text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9707e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_resume(file_path):\n",
    "#     ext = file_path.split(\".\")[-1].lower()\n",
    "#     if ext == \"pdf\":\n",
    "#         resume_text = extract_text_from_file(file_path, \"pdf\")\n",
    "#     elif ext == \"docx\":\n",
    "#         resume_text = extract_text_from_file(file_path, \"docx\")\n",
    "#     else:\n",
    "#         raise ValueError(\"Unsupported file type!\")\n",
    "\n",
    "#     structured_data, latency, prompt_tokens, response_tokens = parse_resume_with_gemini(resume_text)\n",
    "\n",
    "#     log = {\n",
    "#         \"timestamp\": datetime.now().isoformat(),\n",
    "#         \"file\": file_path,\n",
    "#         \"latency_sec\": round(latency, 2),\n",
    "#         \"prompt_tokens\": prompt_tokens,\n",
    "#         \"response_tokens\": response_tokens,\n",
    "#         \"output\": structured_data\n",
    "#     }\n",
    "\n",
    "#     return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b930757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume_path = \"resumes/AnupamSharma.pdf\"\n",
    "# result = process_resume(resume_path)\n",
    "\n",
    "# print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bf6a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_path = \"extraction_outputs/output_approach1.json\"\n",
    "\n",
    "# # Create directory if not exists\n",
    "# import os\n",
    "# os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# # Write to JSON file\n",
    "# with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# print(f\"\\n✅ Resume data saved successfully to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f06375f",
   "metadata": {},
   "source": [
    "### Approach 2 – Text → spaCy → LLM\n",
    "- Extract text\n",
    "\n",
    "- Run spaCy NER + rules to pre-parse fields\n",
    "\n",
    "- Send spaCy output to LLM for final structuring\n",
    "\n",
    "- Log metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b63ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f13d375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_with_spacy(text: str):\n",
    "#     doc = nlp(text)\n",
    "#     pre_data = {\n",
    "#         \"name\": None,\n",
    "#         \"email\": None,\n",
    "#         \"phone\": None,\n",
    "#         \"location\": None,\n",
    "#         \"entities\": [],\n",
    "#         \"sections\": {}\n",
    "#     }\n",
    "\n",
    "#     # --- Named Entities ---\n",
    "#     for ent in doc.ents:\n",
    "#         pre_data[\"entities\"].append({\"text\": ent.text, \"label\": ent.label_})\n",
    "#         if ent.label_ == \"PERSON\" and not pre_data[\"name\"]:\n",
    "#             pre_data[\"name\"] = ent.text\n",
    "#         if ent.label_ in [\"GPE\", \"LOC\"] and not pre_data[\"location\"]:\n",
    "#             pre_data[\"location\"] = ent.text\n",
    "\n",
    "#     # --- Regex extraction ---\n",
    "#     email_match = re.search(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", text)\n",
    "#     phone_match = re.search(r\"\\+?\\d[\\d\\s\\-]{7,}\\d\", text)\n",
    "#     pre_data[\"email\"] = email_match.group() if email_match else None\n",
    "#     pre_data[\"phone\"] = phone_match.group() if phone_match else None\n",
    "\n",
    "#     # --- Section Splitting ---\n",
    "#     section_titles = [\n",
    "#         \"Education\", \"Experience\", \"Projects\", \"Certifications\",\n",
    "#         \"Technical Skills\", \"Skills\", \"Summary\", \"Profile\", \"Objective\"\n",
    "#     ]\n",
    "#     pattern = r\"(?i)\\b(\" + \"|\".join(section_titles) + r\")\\b[:\\s]?\"\n",
    "#     sections = re.split(pattern, text)\n",
    "#     for i in range(1, len(sections), 2):\n",
    "#         header = sections[i].strip().title()\n",
    "#         content = sections[i + 1].strip() if i + 1 < len(sections) else \"\"\n",
    "#         pre_data[\"sections\"][header] = content[:2000]  # limit long text\n",
    "\n",
    "#     return pre_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f57373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_resume_with_gemini(resume_text, pre_data):\n",
    "#     prompt = f\"\"\"You are an expert Resume Intelligence Agent that parses resumes into structured JSON.\n",
    "# Below is the raw resume text and the pre-parsed information from spaCy and regex.\n",
    "\n",
    "# Use the pre-parsed data to enhance extraction accuracy.\n",
    "\n",
    "# Return ONLY a valid JSON matching this schema:\n",
    "\n",
    "# {{\n",
    "#   \"name\": \"\",\n",
    "#   \"first_name\": \"\",\n",
    "#   \"last_name\": \"\",\n",
    "#   \"email_address\": \"\",\n",
    "#   \"mobile_number\": \"\",\n",
    "#   \"country_code\": \"\",\n",
    "#   \"location\": \"\",\n",
    "#   \"city\": \"\",\n",
    "#   \"state\": \"\",\n",
    "#   \"country\": \"\",\n",
    "#   \"zip_postal_code\": \"\",\n",
    "#   \"summary\": \"\",\n",
    "#   \"skills\": [],\n",
    "#   \"extra_skills\": [],\n",
    "#   \"work_experience\": [],\n",
    "#   \"projects\": [],\n",
    "#   \"certifications\": [],\n",
    "#   \"education\": [],\n",
    "#   \"experience_level\": \"\",\n",
    "#   \"experience_level_number\": 0,\n",
    "#   \"role_keywords\": [],\n",
    "#   \"tech_stack_summary\": \"\",\n",
    "#   \"current_employer\": \"\",\n",
    "#   \"experience_by_skill\": {{}},\n",
    "#   \"availability_questions\": {{\n",
    "#     \"own_laptop_for_evaluation\": \"Yes\",\n",
    "#     \"willing_for_2_3_month_evaluation\": \"Yes\",\n",
    "#     \"available_for_fulltime_internship\": \"Yes\",\n",
    "#     \"preferred_work_timings\": \"Flexible\",\n",
    "#     \"can_start_immediately\": \"Yes\",\n",
    "#     \"work_authorization\": \"Yes\"\n",
    "#   }}\n",
    "# }}\n",
    "\n",
    "# CRITICAL EXTRACTION RULES FOR ALL SECTIONS:\n",
    "\n",
    "# 1. **NAME / FIRST_NAME / LAST_NAME**:  \n",
    "#    - Extract full name exactly as written (usually top of resume).  \n",
    "#    - Split logically into first_name and last_name; if single name, leave last_name empty.\n",
    "\n",
    "# 2. **EMAIL_ADDRESS / MOBILE_NUMBER / COUNTRY_CODE**:  \n",
    "#    - Extract directly from text.  \n",
    "#    - Derive `country_code` if phone number includes it (e.g., \"+91\"), otherwise leave empty.\n",
    "\n",
    "# 3. **LOCATION / CITY / STATE / COUNTRY / ZIP_POSTAL_CODE**:  \n",
    "#    - Extract location only from the contact section.  \n",
    "#    - Derive state/country if missing, e.g., \"Bangalore\" → \"Karnataka, India\".  \n",
    "#    - If city is present but ZIP/postal code is missing, **infer it automatically using known mapping for that city** (e.g., Bangalore → \"560001\").  \n",
    "#    - If city is absent, leave ZIP/postal code empty.  \n",
    "#    - Never use locations mentioned under work experience or projects.\n",
    "\n",
    "# 4. **SUMMARY**:  \n",
    "#    - Extract text from “Summary”, “Profile”, “Objective”, or “About Me” sections.\n",
    "\n",
    "# 5. **SKILLS / EXTRA_SKILLS**:  \n",
    "#    - `skills`: primary professional and technical competencies (normalize capitalization).  \n",
    "#    - `extra_skills`: secondary, contextual, or soft skills (avoid duplicates).\n",
    "\n",
    "# 6. **WORK_EXPERIENCE**:  \n",
    "#    - Each record must include title, company, start_date, end_date (or \"Present\"), duration, and key achievements.  \n",
    "#    - **Set `current_employer` only if the role’s end_date is marked as “Present” or “Ongoing”**.  \n",
    "#    - If all roles are completed (no ongoing job), `current_employer` must be an empty string.\n",
    "\n",
    "# 7. **PROJECTS**:  \n",
    "#    - Each must include name, duration, tech_stack, and short description.  \n",
    "#    - Prefer projects from “Projects” or “Academic Work” sections.\n",
    "\n",
    "# 8. **CERTIFICATIONS**:  \n",
    "#    - Include certification/course name, issuing body, and year if present.\n",
    "\n",
    "# 9. **EDUCATION**:  \n",
    "#    - Include degree, institution, field, and graduation year.  \n",
    "#    - If currently studying, mark as “Pursuing” or “Ongoing”.\n",
    "\n",
    "# 10. **EXPERIENCE_LEVEL**:  \n",
    "#    - Based on total experience:  \n",
    "#      - Internship → 0 years or studying  \n",
    "#      - Entry Level → 0–1 years  \n",
    "#      - Associate → 1–3 years  \n",
    "#      - Mid-Senior Level → 3–7 years  \n",
    "#      - Director → 7+ years  \n",
    "\n",
    "# 11. **EXPERIENCE_LEVEL_NUMBER**:  \n",
    "#    - Internship → 1  \n",
    "#    - Entry Level → 2  \n",
    "#    - Associate → 3  \n",
    "#    - Mid-Senior Level → 4  \n",
    "#    - Director → 5  \n",
    "\n",
    "# 12. **ROLE_KEYWORDS**:  \n",
    "#    - Extract key professional or domain-related keywords (e.g., “Full Stack Development”, “MLOps”, “Data Engineering”, etc.)\n",
    "\n",
    "# 13. **EXPERIENCE_BY_SKILL**:  \n",
    "#    - Map each detected skill to an estimated **integer number of years** based on resume content.  \n",
    "#    - Round intelligently:  \n",
    "#      - < 1 year → 1  \n",
    "#      - 1–1.5 years → 1  \n",
    "#      - 1.6–2.4 years → 2  \n",
    "#      - and so on (round to nearest integer).  \n",
    "#    - Example:  \n",
    "#      \"experience_by_skill\": {{\"Python\": 2, \"React\": 1, \"Spring Boot\": 2}}  \n",
    "#    - Output must use **numeric values only**, without “months” or “years” suffix.\n",
    "\n",
    "# 14. **TECH_STACK_SUMMARY**:  \n",
    "#    - Combine all technical tools, libraries, and frameworks (from skills + projects + experience) into a concise, comma-separated list.\n",
    "\n",
    "# 15. **INFERENCE RULES**:\n",
    "#    - Normalize capitalization for consistency.  \n",
    "#    - Avoid duplicate entries across fields.  \n",
    "#    - Do not fabricate data.  \n",
    "#    - Infer logically only where reasonable (like ZIP from city).\n",
    "\n",
    "# Return ONLY the JSON object, with no explanations.\n",
    "\n",
    "# Pre-parsed Data:\n",
    "# {json.dumps(pre_data, indent=2)}\n",
    "\n",
    "\n",
    "# Resume Text:\n",
    "# {resume_text}\n",
    "# \"\"\"\n",
    "\n",
    "#     start = time.time()\n",
    "#     response = model.generate_content(prompt)\n",
    "#     latency = time.time() - start\n",
    "\n",
    "#     # --- Clean and Parse Gemini Output ---\n",
    "#     raw_output = response.text.strip()\n",
    "\n",
    "#     # Remove ```json or ``` wrappers if present\n",
    "#     raw_output = re.sub(r\"^```(json)?\", \"\", raw_output)\n",
    "#     raw_output = re.sub(r\"```$\", \"\", raw_output)\n",
    "#     raw_output = raw_output.strip()\n",
    "\n",
    "#     # Try parsing clean JSON\n",
    "#     try:\n",
    "#         structured = json.loads(raw_output)\n",
    "#     except json.JSONDecodeError:\n",
    "#       structured = {\"raw_output\": raw_output}\n",
    "#     return structured, latency, len(prompt.split()), len(response.text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69024403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_resume(file_path):\n",
    "#     if file_path.endswith(\".pdf\"):\n",
    "#         text = extract_text_from_file(file_path, \"pdf\")\n",
    "#     elif file_path.endswith(\".docx\"):\n",
    "#         text = extract_text_from_file(file_path, \"docx\")\n",
    "#     else:\n",
    "#         raise ValueError(\"Unsupported file type!\")\n",
    "    \n",
    "#     pre_data = preprocess_with_spacy(text)\n",
    "#     structured, latency, prompt_tokens, response_tokens = parse_resume_with_gemini(text, pre_data)\n",
    "\n",
    "#     result = {\n",
    "#         \"timestamp\": datetime.now().isoformat(),\n",
    "#         \"file\": file_path,\n",
    "#         \"latency_sec\": latency,\n",
    "#         \"prompt_tokens\": prompt_tokens,\n",
    "#         \"response_tokens\": response_tokens,\n",
    "#         \"output\": structured\n",
    "#     }\n",
    "\n",
    "#     print(json.dumps(result, indent=2))\n",
    "#     return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd0a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume_path = \"resumes/VedantResume.pdf\"  # update path if needed\n",
    "# result2 = process_resume(resume_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ced9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_path = \"extraction_outputs/output_approach2.json\"\n",
    "\n",
    "# # Create directory if not exists\n",
    "# import os\n",
    "# os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# # Write to JSON file\n",
    "# with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(result2, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# print(f\"\\n✅ Resume data saved successfully to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589ee0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from difflib import SequenceMatcher\n",
    "# from tabulate import tabulate\n",
    "# import json\n",
    "\n",
    "# def normalize_list_items(lst):\n",
    "#     \"\"\"Convert all list items (strings or dicts) into comparable lowercase strings.\"\"\"\n",
    "#     normalized = []\n",
    "#     for item in lst:\n",
    "#         if isinstance(item, dict):\n",
    "#             normalized.append(json.dumps(item, sort_keys=True).lower())\n",
    "#         else:\n",
    "#             normalized.append(str(item).lower())\n",
    "#     return normalized\n",
    "\n",
    "# def jaccard_similarity(list1, list2):\n",
    "#     if not list1 and not list2:\n",
    "#         return 1.0\n",
    "#     if not list1 or not list2:\n",
    "#         return 0.0\n",
    "#     set1 = set(normalize_list_items(list1))\n",
    "#     set2 = set(normalize_list_items(list2))\n",
    "#     return len(set1 & set2) / len(set1 | set2)\n",
    "\n",
    "# def text_similarity(a, b):\n",
    "#     if not a or not b:\n",
    "#         return 0.0\n",
    "#     return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "# def compare_resume_outputs(approach1_output, approach2_output):\n",
    "#     fields_to_compare = [\n",
    "#         \"skills\", \"extra_skills\", \"projects\", \"certifications\",\n",
    "#         \"education\", \"role_keywords\", \"tech_stack_summary\",\n",
    "#         \"work_experience\", \"summary\", \"experience_level\",\n",
    "#         \"experience_by_skill\"\n",
    "#     ]\n",
    "\n",
    "#     results = []\n",
    "#     summary = {}\n",
    "\n",
    "#     for field in fields_to_compare:\n",
    "#         val1 = approach1_output.get(field)\n",
    "#         val2 = approach2_output.get(field)\n",
    "#         sim = None\n",
    "\n",
    "#         # --- List-based fields ---\n",
    "#         if isinstance(val1, list) and isinstance(val2, list):\n",
    "#             sim = jaccard_similarity(val1, val2)\n",
    "#             summary[field] = {\n",
    "#                 \"common\": list(set(normalize_list_items(val1)) & set(normalize_list_items(val2))),\n",
    "#                 \"only_in_approach1\": list(set(normalize_list_items(val1)) - set(normalize_list_items(val2))),\n",
    "#                 \"only_in_approach2\": list(set(normalize_list_items(val2)) - set(normalize_list_items(val1))),\n",
    "#                 \"similarity\": round(sim, 2)\n",
    "#             }\n",
    "\n",
    "#         # --- Dict-based fields ---\n",
    "#         elif isinstance(val1, dict) and isinstance(val2, dict):\n",
    "#             common_keys = set(val1.keys()) & set(val2.keys())\n",
    "#             diff1 = set(val1.keys()) - set(val2.keys())\n",
    "#             diff2 = set(val2.keys()) - set(val1.keys())\n",
    "#             sim = len(common_keys) / len(set(val1.keys()) | set(val2.keys()) or [1])\n",
    "#             summary[field] = {\n",
    "#                 \"common_keys\": list(common_keys),\n",
    "#                 \"only_in_approach1\": list(diff1),\n",
    "#                 \"only_in_approach2\": list(diff2),\n",
    "#                 \"similarity\": round(sim, 2)\n",
    "#             }\n",
    "\n",
    "#         # --- String fields ---\n",
    "#         elif isinstance(val1, str) and isinstance(val2, str):\n",
    "#             sim = text_similarity(val1, val2)\n",
    "#             summary[field] = {\"similarity\": round(sim, 2)}\n",
    "\n",
    "#         else:\n",
    "#             summary[field] = {\"similarity\": None}\n",
    "\n",
    "#         results.append([field, round(sim, 2) if sim is not None else \"-\", \"✅\" if sim and sim > 0.7 else \"⚠️\"])\n",
    "\n",
    "#     print(\"\\n📊 **Resume Comparison Summary (Approach 1 vs Approach 2)**\\n\")\n",
    "#     print(tabulate(results, headers=[\"Field\", \"Similarity\", \"Status\"], tablefmt=\"github\"))\n",
    "    \n",
    "#     return summary\n",
    "\n",
    "\n",
    "# # --- Load both outputs ---\n",
    "# with open(\"extraction_outputs/output_approach1.json\", \"r\") as f1:\n",
    "#     approach1_data = json.load(f1)\n",
    "\n",
    "# with open(\"extraction_outputs/output_approach2.json\", \"r\") as f2:\n",
    "#     approach2_data = json.load(f2)\n",
    "\n",
    "# comparison_summary = compare_resume_outputs(\n",
    "#     approach1_data[\"output\"],\n",
    "#     approach2_data[\"output\"]\n",
    "# )\n",
    "\n",
    "# # Save the detailed report\n",
    "# with open(\"comparison_report.json\", \"w\") as f:\n",
    "#     json.dump(comparison_summary, f, indent=2)\n",
    "\n",
    "# print(\"\\n✅ Comparison report saved as comparison_report.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f160b5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.26.5-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.26.5\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90a977b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 23:30:45,071 - INFO - Average font size calculated: 9.63\n",
      "2025-10-27 23:30:45,080 - INFO - Average font size calculated: 9.63\n",
      "2025-10-27 23:30:45,083 - INFO - ✅ Sections saved to resumes/Yeswanth_Yerra_CV_sections.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Section: Education ===\n",
      "Pragati Engineering College\n",
      "Surampalem, AP\n",
      "Bachelor of Technology, Computer Science and Engineering (GPA- 8.14/10.0)\n",
      "Aug. 2019 – May 2023\n",
      "Sasi junior College\n",
      "Eluru, AP\n",
      "Intermediate (GPA-9.8/10.0)\n",
      "Aug 2017 – May 2019\n",
      "\n",
      "\n",
      "=== Section: Technical Skills ===\n",
      "\n",
      "\n",
      "\n",
      "=== Section: Languages ===\n",
      ": Java, JavaScript, Python\n",
      "Core Skills\n",
      ": Data Structures & Algorithms, Computer Networks , Operating Systems,DBMS\n",
      "Frameworks\n",
      ": Spring Boot, Hibernate,React.js,Flask\n",
      "Developer Tools\n",
      ": Git, Docker, Terraform, Ansible, Jenkins\n",
      "Databases\n",
      ": MySQL, MongoDB, PostgreSQL\n",
      "Cloud Computing\n",
      ": AWS,Kubernetes\n",
      "\n",
      "\n",
      "=== Section: Experience ===\n",
      "Full Stack Development Intern\n",
      "June 2024 - October 2024\n",
      "Pantech Prolabs Pvt Ltd\n",
      "Chennai,TN(Remote)\n",
      "•\n",
      "Engineered a web application using Spring Boot for the backend, employing RESTful APIs and efficient database\n",
      "querying with MongoDB.\n",
      "•\n",
      "Gained expertise in design patterns and microservice architecture for building efficient, modular systems.\n",
      "•\n",
      "Applied scalable web development practices to ensure optimal performance under increasing user loads and\n",
      "Implemented authentication and authorization.\n",
      "•\n",
      "Maintained high code quality by adhering to clean coding practices and leveraging testing frameworks like JUnit5\n",
      "and Mockito, achieving 95% test coverage.\n",
      "•\n",
      "Enhanced application reliability with Docker containerization, cutting deployment time by 50% and ensuring\n",
      "consistent performance across all environments.\n",
      "\n",
      "\n",
      "=== Section: Projects ===\n",
      "Movie Recommendation System\n",
      "|\n",
      "Demo\n",
      "October 2021 - March 2022\n",
      "•\n",
      "Led a team of 4 members for movie recommendation system by integrating collaborative filtering, content-based\n",
      "filtering, and matrix factorization, resulting in a 33% improvement in accuracy using the MovieLens dataset.\n",
      "•\n",
      "Extracted metadata features and employed TF-IDF for precise vector representation and enhancing feature quality\n",
      "by 30%.\n",
      "•\n",
      "Reduced RMSE from 2.2489 to 1.1000 by finetuning ALS matrix factorization for collaborative filtering.\n",
      "•\n",
      "Addressed sparsity and cold-start challenges effectively using content-based filtering, reducing cold-start failures by\n",
      "50% and improving recommendation coverage.\n",
      "Network Intrusion Detection System (NIDS)\n",
      "|\n",
      "Repo\n",
      "October 2022 - March 2023\n",
      "•\n",
      "Engineered a Network Intrusion Detection System Utilizing BiLSTM to capture both forward and backward\n",
      "dependencies,Integrated an attention layer to prioritize critical features, resulting in a 25% increase in detection\n",
      "accuracy.\n",
      "•\n",
      "Trained the model on the refined NSL-KDD dataset, addressing class imbalance and boosting overall performance\n",
      "by 30%.\n",
      "•\n",
      "Implemented hyperparameter tuning using grid search and random search, optimizing the BiLSTM network’s\n",
      "learning rate, batch size, and number of hidden layers, leading to a 20% increase in accuracy.\n",
      "Job Application Microservices API\n",
      "|\n",
      "Repo\n",
      "October 2023 - December 2023\n",
      "•\n",
      "Designed and deployed a microservices architecture using Spring Boot, Eureka, and Feign clients, enabling seamless\n",
      "communication between 3+ services, with 99.9% availability.\n",
      "•\n",
      "Implemented JWT-based authentication and role-based security across 3 microservices, securing 100+ endpoints\n",
      "with 2 or more roles per service.\n",
      "•\n",
      "Integrated RabbitMQ for asynchronous communication, handling up to 50,000 messages per day between\n",
      "microservices, improving system performance and reliability.\n",
      "•\n",
      "Deployed the application on Kubernetes, leveraging auto-scaling and caching strategies to efficiently handle traffic\n",
      "spikes and ensure high availability for up to 100,000 concurrent users.\n",
      "Simple Song Search\n",
      "|\n",
      "Repo\n",
      "November 2024 -December 2024\n",
      "•\n",
      "Engineered a robust song search application using Flask and advanced algorithms like TF-IDF and cosine similarity\n",
      "across a dataset of over 50,000 tracks\n",
      "•\n",
      "Improving search speed and accuracy by 40% .\n",
      "•\n",
      "Designed a responsive web interface and deployed the application on Render,achieving a 99.9% deployment success\n",
      "rate.\n",
      "\n",
      "\n",
      "=== Section: Certifications ===\n",
      "DataScience for Engineers\n",
      "- NPTEL-IIT MADRAS\n",
      "•\n",
      "2023\n",
      "Software Engineering Job Simulation\n",
      "- Hewlett Packard Enterprise(Forage)\n",
      "•\n",
      "2024\n",
      "GenAI path in Google Cloud Platform\n",
      "- Google\n",
      "Java 17 MasterClass\n",
      "- Udemy\n",
      "Machine Learning Using Python\n",
      "- SimpliLearn\n",
      "Accounting and Data Analytics with Python\n",
      "- Coursera-Illinois University\n",
      "\n",
      "\n",
      "=== Section: Personal Info ===\n",
      "Yeswanth Yerra\n",
      "+91 9550413132\n",
      "|\n",
      "yeswanthyerra07@gmail.com\n",
      "|\n",
      "Linkedin\n",
      "|\n",
      "Github\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import re\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# =========================\n",
    "# Logging setup\n",
    "# =========================\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helper: Calculate average font size\n",
    "# =========================\n",
    "def calculate_average_font_size(doc):\n",
    "    total_font_size = 0\n",
    "    num_fonts = 0\n",
    "    for page in doc:\n",
    "        blocks = page.get_text(\"dict\")['blocks']\n",
    "        for block in blocks:\n",
    "            if block['type'] == 0:  # text block\n",
    "                for line in block.get('lines', []):\n",
    "                    for span in line.get('spans', []):\n",
    "                        total_font_size += span['size']\n",
    "                        num_fonts += 1\n",
    "    return total_font_size / num_fonts if num_fonts > 0 else 12  # fallback avg font size\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Extract resume sections\n",
    "# =========================\n",
    "def extract_sections(pdf_path, headings=None):\n",
    "    if headings is None:\n",
    "        headings = [\n",
    "            r\"Education\", r\"Work Experience\", r\"Professional Experience\", r\"Experience\", r\"Projects\",\n",
    "            r\"Skills\", r\"Certifications\", r\"Summary\", r\"Contact\", r\"Technical Skills\",\n",
    "            r\"Experience\", r\"Location\" , r\"Extra Curricular Activities\" , r\"Languages\"\n",
    "        ]\n",
    "\n",
    "    heading_pattern = re.compile(r\"^(\" + r\"|\".join(headings) + r\")\\s*$\", flags=re.IGNORECASE)\n",
    "\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to open PDF: {e}\")\n",
    "        return {}\n",
    "\n",
    "    average_font_size = calculate_average_font_size(doc)\n",
    "    logging.info(f\"Average font size calculated: {average_font_size:.2f}\")\n",
    "\n",
    "    sections = {}\n",
    "    current_heading = None\n",
    "    current_section_text = []\n",
    "\n",
    "    for page in doc:\n",
    "        blocks = page.get_text(\"dict\")['blocks']\n",
    "        for block in blocks:\n",
    "            if block['type'] == 0:  # Only text blocks\n",
    "                for line in block.get('lines', []):\n",
    "                    for span in line.get('spans', []):\n",
    "                        text = span['text'].strip()\n",
    "                        if not text:\n",
    "                            continue\n",
    "\n",
    "                        font_size = span.get('size', 0)\n",
    "\n",
    "                        # Heading detection\n",
    "                        if heading_pattern.match(text) or font_size > 1.5 * average_font_size:\n",
    "                            if current_heading:\n",
    "                                sections[current_heading] = \"\\n\".join(current_section_text).strip()\n",
    "                            current_heading = text.strip().title()\n",
    "                            current_section_text = []\n",
    "                        elif current_heading:\n",
    "                            current_section_text.append(text)\n",
    "\n",
    "    # Add last section\n",
    "    if current_heading:\n",
    "        sections[current_heading] = \"\\n\".join(current_section_text).strip()\n",
    "\n",
    "    # =========================\n",
    "    # Rename top heading → Personal Info\n",
    "    # =========================\n",
    "    if sections:\n",
    "        first_key = list(sections.keys())[0]\n",
    "        first_value = sections[first_key]\n",
    "        # Append name to value and rename key\n",
    "        sections[\"Personal Info\"] = f\"{first_key}\\n{first_value}\".strip()\n",
    "        del sections[first_key]\n",
    "        \n",
    "    return sections\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Save JSON\n",
    "# =========================\n",
    "def save_sections_to_json(pdf_path, sections):\n",
    "    # --- Start of Modification ---\n",
    "\n",
    "    # Hardcoded Key-Value Pairs\n",
    "    # Create a new dictionary to hold the sections and the hardcoded values\n",
    "    output_data = sections.copy() \n",
    "    output_data[\"preferred_title\"] = \"Machine Learning Engineer\"\n",
    "    output_data[\"preferred_job_location\"] = \"India\"\n",
    "    \n",
    "    # --- End of Modification ---\n",
    "\n",
    "    output_path = pdf_path.replace(\".pdf\", \"_sections.json\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        # Save the new dictionary (output_data) instead of the original sections\n",
    "        json.dump(output_data, f, indent=4, ensure_ascii=False)\n",
    "    logging.info(f\"✅ Sections saved to {output_path}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Print sections for debugging\n",
    "# =========================\n",
    "def print_sections(pdf_path):\n",
    "    sections = extract_sections(pdf_path)\n",
    "    if not sections:\n",
    "        logging.error(f\"No sections extracted from {pdf_path}\")\n",
    "        return\n",
    "    for section, content in sections.items():\n",
    "        print(f\"=== Section: {section} ===\")\n",
    "        print(content)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Run Example\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"resumes/Yeswanth_Yerra_CV.pdf\"  # update your path\n",
    "    # NOTE: print_sections uses extract_sections directly and won't show the hardcoded values\n",
    "    print_sections(pdf_path) \n",
    "    \n",
    "    # Extract sections, then save including the hardcoded values\n",
    "    sections = extract_sections(pdf_path)\n",
    "    save_sections_to_json(pdf_path, sections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e8e2238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 23:32:13,273 - INFO - 📏 Average font size: 9.63\n",
      "2025-10-27 23:32:13,282 - INFO - 📏 Average font size: 9.63\n",
      "2025-10-27 23:32:13,286 - INFO - ✅ Sections saved to resumes/Yeswanth_Yerra_CV_sections.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Section: Personal Info ===\n",
      "Yeswanth Yerra\n",
      "+91 9550413132  | yeswanthyerra07@gmail.com  |  Linkedin  |  Github\n",
      "\n",
      "\n",
      "=== Section: Education ===\n",
      "Pragati Engineering College\n",
      "Surampalem, AP\n",
      "Bachelor of Technology, Computer Science and Engineering (GPA- 8.14/10.0)\n",
      "Aug. 2019 – May 2023\n",
      "Sasi junior College\n",
      "Eluru, AP\n",
      "Intermediate (GPA-9.8/10.0)\n",
      "Aug 2017 – May 2019\n",
      "\n",
      "\n",
      "=== Section: Technical Skills ===\n",
      "Languages : Java, JavaScript, Python\n",
      "Core Skills : Data Structures & Algorithms, Computer Networks , Operating Systems,DBMS\n",
      "Frameworks : Spring Boot, Hibernate,React.js,Flask\n",
      "Developer Tools : Git, Docker, Terraform, Ansible, Jenkins\n",
      "Databases : MySQL, MongoDB, PostgreSQL\n",
      "Cloud Computing : AWS,Kubernetes\n",
      "\n",
      "\n",
      "=== Section: Experience ===\n",
      "Full Stack Development Intern\n",
      "June 2024 - October 2024\n",
      "Pantech Prolabs Pvt Ltd\n",
      "Chennai,TN(Remote)\n",
      "•  Engineered a web application using Spring Boot for the backend, employing RESTful APIs and efficient database\n",
      "querying with MongoDB.\n",
      "•  Gained expertise in design patterns and microservice architecture for building efficient, modular systems.\n",
      "•  Applied scalable web development practices to ensure optimal performance under increasing user loads and\n",
      "Implemented authentication and authorization.\n",
      "•  Maintained high code quality by adhering to clean coding practices and leveraging testing frameworks like JUnit5\n",
      "and Mockito, achieving 95% test coverage.\n",
      "•  Enhanced application reliability with Docker containerization, cutting deployment time by 50% and ensuring\n",
      "consistent performance across all environments.\n",
      "\n",
      "\n",
      "=== Section: Projects ===\n",
      "Movie Recommendation System  |  Demo\n",
      "October 2021 - March 2022\n",
      "•  Led a team of 4 members for movie recommendation system by integrating collaborative filtering, content-based\n",
      "filtering, and matrix factorization, resulting in a 33% improvement in accuracy using the MovieLens dataset.\n",
      "•  Extracted metadata features and employed TF-IDF for precise vector representation and enhancing feature quality\n",
      "by 30%.\n",
      "•  Reduced RMSE from 2.2489 to 1.1000 by finetuning ALS matrix factorization for collaborative filtering.\n",
      "•  Addressed sparsity and cold-start challenges effectively using content-based filtering, reducing cold-start failures by\n",
      "50% and improving recommendation coverage.\n",
      "Network Intrusion Detection System (NIDS)  |  Repo\n",
      "October 2022 - March 2023\n",
      "•  Engineered a Network Intrusion Detection System Utilizing BiLSTM to capture both forward and backward\n",
      "dependencies,Integrated an attention layer to prioritize critical features, resulting in a 25% increase in detection\n",
      "accuracy.\n",
      "•  Trained the model on the refined NSL-KDD dataset, addressing class imbalance and boosting overall performance\n",
      "by 30%.\n",
      "•  Implemented hyperparameter tuning using grid search and random search, optimizing the BiLSTM network’s\n",
      "learning rate, batch size, and number of hidden layers, leading to a 20% increase in accuracy.\n",
      "Job Application Microservices API  |  Repo\n",
      "October 2023 - December 2023\n",
      "•  Designed and deployed a microservices architecture using Spring Boot, Eureka, and Feign clients, enabling seamless\n",
      "communication between 3+ services, with 99.9% availability.\n",
      "•  Implemented JWT-based authentication and role-based security across 3 microservices, securing 100+ endpoints\n",
      "with 2 or more roles per service.\n",
      "•  Integrated RabbitMQ for asynchronous communication, handling up to 50,000 messages per day between\n",
      "microservices, improving system performance and reliability.\n",
      "•  Deployed the application on Kubernetes, leveraging auto-scaling and caching strategies to efficiently handle traffic\n",
      "spikes and ensure high availability for up to 100,000 concurrent users.\n",
      "Simple Song Search  |  Repo\n",
      "November 2024 -December 2024\n",
      "•  Engineered a robust song search application using Flask and advanced algorithms like TF-IDF and cosine similarity\n",
      "across a dataset of over 50,000 tracks\n",
      "•  Improving search speed and accuracy by 40% .\n",
      "•  Designed a responsive web interface and deployed the application on Render,achieving a 99.9% deployment success\n",
      "rate.\n",
      "\n",
      "\n",
      "=== Section: Certifications ===\n",
      "DataScience for Engineers  - NPTEL-IIT MADRAS  •  2023\n",
      "Software Engineering Job Simulation  - Hewlett Packard Enterprise(Forage)  •  2024\n",
      "GenAI path in Google Cloud Platform  - Google\n",
      "Java 17 MasterClass  - Udemy\n",
      "Machine Learning Using Python  - SimpliLearn\n",
      "Accounting and Data Analytics with Python  - Coursera-Illinois University\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import re\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# =========================\n",
    "# Logging setup\n",
    "# =========================\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helper: Calculate average font size\n",
    "# =========================\n",
    "def calculate_average_font_size(doc):\n",
    "    total_font_size = 0\n",
    "    num_fonts = 0\n",
    "    for page in doc:\n",
    "        blocks = page.get_text(\"dict\")['blocks']\n",
    "        for block in blocks:\n",
    "            if block['type'] == 0:  # text block\n",
    "                for line in block.get('lines', []):\n",
    "                    for span in line.get('spans', []):\n",
    "                        total_font_size += span['size']\n",
    "                        num_fonts += 1\n",
    "    return total_font_size / num_fonts if num_fonts > 0 else 12\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Extract resume sections\n",
    "# =========================\n",
    "def extract_sections(pdf_path, headings=None):\n",
    "    if headings is None:\n",
    "        headings = [\n",
    "            r\"Objective\", r\"Summary\", r\"Education\", r\"Work Experience\", r\"Professional Experience\",\n",
    "            r\"Projects\", r\"Skills\", r\"Certifications\", r\"Technical Skills\", r\"Experience\",\n",
    "            r\"Achievements\", r\"Internship\", r\"Hobbies\", r\"Interests\"\n",
    "        ]\n",
    "\n",
    "    heading_pattern = re.compile(r\"^(\" + r\"|\".join(headings) + r\")\\s*$\", flags=re.IGNORECASE)\n",
    "    separator_pattern = re.compile(r\"^[-–—_=#*]{3,}$\")  # e.g. \"----\" or \"=====\" or \"____\"\n",
    "\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to open PDF: {e}\")\n",
    "        return {}\n",
    "\n",
    "    avg_font = calculate_average_font_size(doc)\n",
    "    logging.info(f\"📏 Average font size: {avg_font:.2f}\")\n",
    "\n",
    "    # Extract all lines + font sizes\n",
    "    lines = []\n",
    "    for page in doc:\n",
    "        for block in page.get_text(\"dict\")['blocks']:\n",
    "            if block['type'] != 0:\n",
    "                continue\n",
    "            for line in block.get('lines', []):\n",
    "                text = \" \".join(span.get(\"text\", \"\") for span in line.get(\"spans\", [])).strip()\n",
    "                if text:\n",
    "                    max_font = max(span.get(\"size\", 0) for span in line.get(\"spans\", []))\n",
    "                    lines.append((text, max_font))\n",
    "\n",
    "    # Detect heading-like lines\n",
    "    heading_indices = []\n",
    "    for idx, (text, size) in enumerate(lines):\n",
    "        if heading_pattern.match(text) or size > 1.5 * avg_font:\n",
    "            heading_indices.append(idx)\n",
    "\n",
    "    # --- Smart boundary detection for Personal Info ---\n",
    "    personal_info_lines = []\n",
    "    boundary_index = len(lines)  # default end\n",
    "\n",
    "    # Case 1: Stop before second heading (if present)\n",
    "    if len(heading_indices) >= 2:\n",
    "        boundary_index = heading_indices[1]\n",
    "\n",
    "    # Case 2: Stop at separator line (if present before boundary)\n",
    "    for idx, (text, _) in enumerate(lines[:boundary_index]):\n",
    "        if separator_pattern.match(text):\n",
    "            boundary_index = idx\n",
    "            break\n",
    "\n",
    "    # Extract Personal Info\n",
    "    for text, _ in lines[:boundary_index]:\n",
    "        personal_info_lines.append(text)\n",
    "    personal_info_text = \"\\n\".join(personal_info_lines).strip()\n",
    "\n",
    "    # Initialize sections dict\n",
    "    sections = {\"Personal Info\": personal_info_text}\n",
    "\n",
    "    # --- Extract remaining sections ---\n",
    "    current_heading = None\n",
    "    current_text = []\n",
    "    for idx, (text, size) in enumerate(lines[boundary_index:], start=boundary_index):\n",
    "        if heading_pattern.match(text) or size > 1.5 * avg_font:\n",
    "            if current_heading:\n",
    "                sections[current_heading] = \"\\n\".join(current_text).strip()\n",
    "            current_heading = text.strip().title()\n",
    "            current_text = []\n",
    "        elif current_heading:\n",
    "            current_text.append(text)\n",
    "\n",
    "    # Add last section\n",
    "    if current_heading:\n",
    "        sections[current_heading] = \"\\n\".join(current_text).strip()\n",
    "\n",
    "    return sections\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Save JSON\n",
    "# =========================\n",
    "def save_sections_to_json(pdf_path, sections):\n",
    "    # --- Start of Modification ---\n",
    "\n",
    "    # Hardcoded Key-Value Pairs\n",
    "    # Create a new dictionary to hold the sections and the hardcoded values\n",
    "    output_data = sections.copy() \n",
    "    output_data[\"preferred_title\"] = \"Machine Learning Engineer\"\n",
    "    output_data[\"preferred_job_location\"] = \"India\"\n",
    "    \n",
    "    # --- End of Modification ---\n",
    "\n",
    "    output_path = pdf_path.replace(\".pdf\", \"_sections.json\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        # Save the new dictionary (output_data) instead of the original sections\n",
    "        json.dump(output_data, f, indent=4, ensure_ascii=False)\n",
    "    logging.info(f\"✅ Sections saved to {output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Debug print\n",
    "# =========================\n",
    "def print_sections(pdf_path):\n",
    "    sections = extract_sections(pdf_path)\n",
    "    for section, content in sections.items():\n",
    "        print(f\"\\n=== Section: {section} ===\\n{content}\\n\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Run Example\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"resumes/Yeswanth_Yerra_CV.pdf\"\n",
    "    sections = extract_sections(pdf_path)\n",
    "    print_sections(pdf_path)\n",
    "    save_sections_to_json(pdf_path, sections)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bce2d9",
   "metadata": {},
   "source": [
    "## Final Tweaked Resume Extraction Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d66b830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 23:33:29,885 - INFO - 🧠 Name candidate (largest font): Yeswanth Yerra\n",
      "2025-10-27 23:33:29,892 - INFO - 📏 Average font size: 9.63\n",
      "2025-10-27 23:33:29,896 - INFO - ✅ Extracted structured resume saved to: resumes/Yeswanth_Yerra_CV_structured.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Personal Info\": {\n",
      "        \"name\": \"Yeswanth Yerra\",\n",
      "        \"emails\": [\n",
      "            \"yeswanthyerra07@gmail.com\"\n",
      "        ],\n",
      "        \"phones\": [\n",
      "            \"+91 9550413132\"\n",
      "        ],\n",
      "        \"links\": [],\n",
      "        \"raw\": \"['Yeswanth Yerra', '+91 9550413132 yeswanthyerra07@gmail.com Linkedin Github', 'Education', 'Pragati Engineering College', 'Surampalem, AP', 'Bachelor of Technology, Computer Science and Engineering (GPA- 8.14/10.0)', 'Aug. 2019 May 2023', 'Sasi junior College', 'Eluru, AP', 'Intermediate (GPA-9.8/10.0)']\"\n",
      "    },\n",
      "    \"Yeswanth Yerra\": \"+91 9550413132 yeswanthyerra07@gmail.com Linkedin Github\",\n",
      "    \"Education\": \"Pragati Engineering College\\nSurampalem, AP\\nBachelor of Technology, Computer Science and Engineering (GPA- 8.14/10.0)\\nAug. 2019 May 2023\\nSasi junior College\\nEluru, AP\\nIntermediate (GPA-9.8/10.0)\\nAug 2017 May 2019\",\n",
      "    \"Technical Skills\": \"Languages: Java, JavaScript, Python\\nCore Skills: Data Structures & Algorithms, Computer Networks , Operating Systems,DBMS\\nFrameworks: Spring Boot, Hibernate,React.js,Flask\\nDeveloper Tools: Git, Docker, Terraform, Ansible, Jenkins\\nDatabases: MySQL, MongoDB, PostgreSQL\\nCloud Computing: AWS,Kubernetes\",\n",
      "    \"Experience\": \"Full Stack Development Intern\\nJune 2024 - October 2024\\nPantech Prolabs Pvt Ltd\\nChennai,TN(Remote) Engineered a web application using Spring Boot for the backend, employing RESTful APIs and efficient database\\nquerying with MongoDB. Gained expertise in design patterns and microservice architecture for building efficient, modular systems. Applied scalable web development practices to ensure optimal performance under increasing user loads and\\nImplemented authentication and authorization. Maintained high code quality by adhering to clean coding practices and leveraging testing frameworks like JUnit5\\nand Mockito, achieving 95% test coverage. Enhanced application reliability with Docker containerization, cutting deployment time by 50% and ensuring\\nconsistent performance across all environments.\",\n",
      "    \"Projects\": \"Movie Recommendation System Demo\\nOctober 2021 - March 2022 Led a team of 4 members for movie recommendation system by integrating collaborative filtering, content-based\\nfiltering, and matrix factorization, resulting in a 33% improvement in accuracy using the MovieLens dataset. Extracted metadata features and employed TF-IDF for precise vector representation and enhancing feature quality\\nby 30%. Reduced RMSE from 2.2489 to 1.1000 by finetuning ALS matrix factorization for collaborative filtering. Addressed sparsity and cold-start challenges effectively using content-based filtering, reducing cold-start failures by\\n50% and improving recommendation coverage.\\nNetwork Intrusion Detection System (NIDS) Repo\\nOctober 2022 - March 2023 Engineered a Network Intrusion Detection System Utilizing BiLSTM to capture both forward and backward\\ndependencies,Integrated an attention layer to prioritize critical features, resulting in a 25% increase in detection\\naccuracy. Trained the model on the refined NSL-KDD dataset, addressing class imbalance and boosting overall performance\\nby 30%. Implemented hyperparameter tuning using grid search and random search, optimizing the BiLSTM networks\\nlearning rate, batch size, and number of hidden layers, leading to a 20% increase in accuracy.\\nJob Application Microservices API Repo\\nOctober 2023 - December 2023 Designed and deployed a microservices architecture using Spring Boot, Eureka, and Feign clients, enabling seamless\\ncommunication between 3+ services, with 99.9% availability. Implemented JWT-based authentication and role-based security across 3 microservices, securing 100+ endpoints\\nwith 2 or more roles per service. Integrated RabbitMQ for asynchronous communication, handling up to 50,000 messages per day between\\nmicroservices, improving system performance and reliability. Deployed the application on Kubernetes, leveraging auto-scaling and caching strategies to efficiently handle traffic\\nspikes and ensure high availability for up to 100,000 concurrent users.\\nSimple Song Search Repo\\nNovember 2024 -December 2024 Engineered a robust song search application using Flask and advanced algorithms like TF-IDF and cosine similarity\\nacross a dataset of over 50,000 tracks Improving search speed and accuracy by 40% . Designed a responsive web interface and deployed the application on Render,achieving a 99.9% deployment success\\nrate.\",\n",
      "    \"Certifications\": \"DataScience for Engineers - NPTEL-IIT MADRAS 2023\\nSoftware Engineering Job Simulation - Hewlett Packard Enterprise(Forage) 2024\\nGenAI path in Google Cloud Platform - Google\\nJava 17 MasterClass - Udemy\\nMachine Learning Using Python - SimpliLearn\\nAccounting and Data Analytics with Python - Coursera-Illinois University\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import unicodedata\n",
    "from typing import Dict, List\n",
    "\n",
    "# =========================\n",
    "# Logger Setup\n",
    "# =========================\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# =========================\n",
    "# Regex Patterns\n",
    "# =========================\n",
    "EMAIL_RE = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n",
    "# Improved phone regex: ensures 10–15 digits, avoids years and short numbers\n",
    "PHONE_RE = re.compile(r\"(\\+?\\d[\\d\\s\\-\\(\\)]{8,}\\d)\")\n",
    "LINK_RE = re.compile(\n",
    "    r\"(?:https?://)?(?:www\\.)?(?:linkedin|github|portfolio|medium|personal|behance)\\.[^\\s,]+\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Text Utilities\n",
    "# =========================\n",
    "def clean_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = re.sub(r\"[^\\x20-\\x7E\\n]+\", \"\", text)\n",
    "    text = re.sub(r\"[•●–~►|#]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Font Analysis\n",
    "# =========================\n",
    "def calculate_average_font_size(doc: fitz.Document) -> float:\n",
    "    total, count = 0, 0\n",
    "    for page in doc:\n",
    "        for block in page.get_text(\"dict\")[\"blocks\"]:\n",
    "            if block.get(\"type\") == 0:\n",
    "                for line in block.get(\"lines\", []):\n",
    "                    for span in line.get(\"spans\", []):\n",
    "                        total += span.get(\"size\", 0)\n",
    "                        count += 1\n",
    "    return total / count if count else 12\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Step 1: Name Extraction\n",
    "# =========================\n",
    "def extract_name_from_font(page: fitz.Page) -> str:\n",
    "    \"\"\"Detect candidate's name using largest font on the first page.\"\"\"\n",
    "    max_font = 0\n",
    "    name_candidate = \"\"\n",
    "    for block in page.get_text(\"dict\")[\"blocks\"]:\n",
    "        if block.get(\"type\") != 0:\n",
    "            continue\n",
    "        for line in block.get(\"lines\", []):\n",
    "            for span in line.get(\"spans\", []):\n",
    "                if span[\"size\"] > max_font:\n",
    "                    max_font = span[\"size\"]\n",
    "                    name_candidate = span[\"text\"].strip()\n",
    "    logging.info(f\"🧠 Name candidate (largest font): {name_candidate}\")\n",
    "    return name_candidate\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Step 2: Contact Info Extraction (Improved)\n",
    "# =========================\n",
    "def extract_contact_info(text: str) -> Dict:\n",
    "    emails = EMAIL_RE.findall(text)\n",
    "    raw_phones = [m.group(0).strip() for m in PHONE_RE.finditer(text)]\n",
    "    links = LINK_RE.findall(text)\n",
    "\n",
    "    # Filter out false positives like \"2018-2019\" or \"2022-present\"\n",
    "    phones = []\n",
    "    for p in raw_phones:\n",
    "        digits = re.sub(r\"\\D\", \"\", p)  # keep only digits\n",
    "        if len(digits) < 9 or len(digits) > 15:  # ignore short/long sequences\n",
    "            continue\n",
    "        if re.match(r\"20\\d{2}\", digits[:4]):  # avoid year-like numbers\n",
    "            continue\n",
    "        phones.append(p.strip())\n",
    "\n",
    "    return {\n",
    "        \"emails\": list(set(emails)),\n",
    "        \"phones\": list(set(phones)),\n",
    "        \"links\": list(set(links)),\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Step 3: Section Extraction\n",
    "# =========================\n",
    "def extract_sections_from_resume(pdf_path: str, headings: List[str] = None) -> Dict:\n",
    "    if headings is None:\n",
    "        headings = [\n",
    "            r\"Objective\", r\"Summary\", r\"Education\", r\"Experience\", r\"Work Experience\",\n",
    "            r\"Professional Experience\", r\"Projects\", r\"Skills\", r\"Technical Skills\",\n",
    "            r\"Certifications\", r\"Internship\", r\"Achievements\", r\"Hobbies\", r\"Interests\"\n",
    "        ]\n",
    "\n",
    "    heading_pattern = re.compile(r\"^\\s*(\" + r\"|\".join(headings) + r\")\\s*$\", re.IGNORECASE)\n",
    "\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ Failed to open PDF: {e}\")\n",
    "        return {}\n",
    "\n",
    "    avg_font = calculate_average_font_size(doc)\n",
    "    logging.info(f\"📏 Average font size: {avg_font:.2f}\")\n",
    "\n",
    "    sections = {}\n",
    "    current_heading = None\n",
    "    current_text = []\n",
    "    first_heading_seen = False\n",
    "\n",
    "    for page in doc:\n",
    "        for block in page.get_text(\"dict\")[\"blocks\"]:\n",
    "            if block.get(\"type\") != 0:\n",
    "                continue\n",
    "            for line in block.get(\"lines\", []):\n",
    "                line_text = \"\".join(span.get(\"text\", \"\") for span in line.get(\"spans\", [])).strip()\n",
    "                if not line_text:\n",
    "                    continue\n",
    "\n",
    "                max_font = max((span.get(\"size\", 0) for span in line.get(\"spans\", [])), default=0)\n",
    "                is_heading = bool(heading_pattern.match(line_text)) or max_font > 1.5 * avg_font\n",
    "\n",
    "                if is_heading:\n",
    "                    if current_heading:\n",
    "                        sections[current_heading] = clean_text(\"\\n\".join(current_text))\n",
    "                    current_heading = line_text.strip().title()\n",
    "                    current_text = []\n",
    "                    first_heading_seen = True\n",
    "                else:\n",
    "                    if current_heading:\n",
    "                        current_text.append(line_text)\n",
    "\n",
    "    if current_heading:\n",
    "        sections[current_heading] = clean_text(\"\\n\".join(current_text))\n",
    "\n",
    "    return sections\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Step 4: Combine All Logic\n",
    "# =========================\n",
    "def extract_resume_data(pdf_path: str) -> Dict:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    first_page = doc[0]\n",
    "\n",
    "    # Personal Info Region (first page)\n",
    "    first_page_text = first_page.get_text(\"text\")\n",
    "    name = extract_name_from_font(first_page)\n",
    "    contact_info = extract_contact_info(first_page_text)\n",
    "\n",
    "    personal_info = {\n",
    "        \"name\": name,\n",
    "        \"emails\": contact_info.get(\"emails\", []),\n",
    "        \"phones\": contact_info.get(\"phones\", []),\n",
    "        \"links\": contact_info.get(\"links\", []),\n",
    "        \"raw\": clean_text(first_page_text.split(\"\\n\")[0:10].__str__()),\n",
    "    }\n",
    "\n",
    "    # Other structured sections\n",
    "    sections = extract_sections_from_resume(pdf_path)\n",
    "\n",
    "    # Merge into final structure\n",
    "    result = {\"Personal Info\": personal_info}\n",
    "    result.update(sections)\n",
    "    return result\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Step 5: Save to JSON\n",
    "# =========================\n",
    "def save_to_json(pdf_path: str, data: Dict):\n",
    "    # --- Modification Start ---\n",
    "    # Add the hardcoded key-value pairs to the data dictionary\n",
    "    data[\"preferred_title\"] = \"Machine Learning Engineer\"\n",
    "    data[\"preferred_job_location\"] = \"India\"\n",
    "    # --- Modification End ---\n",
    "\n",
    "    output_path = pdf_path.replace(\".pdf\", \"_structured.json\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "    logging.info(f\"✅ Extracted structured resume saved to: {output_path}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Run Example\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"resumes/Yeswanth_Yerra_CV.pdf\"  # update your path\n",
    "    data = extract_resume_data(pdf_path)\n",
    "    print(json.dumps(data, indent=4, ensure_ascii=False))\n",
    "    save_to_json(pdf_path, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6c3c28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-28 07:18:40,456 - INFO - 🧠 Name candidate (largest font): Yeswanth Yerra\n",
      "2025-10-28 07:18:40,464 - INFO -  Average font size: 9.63\n",
      "2025-10-28 07:18:40,473 - INFO - ✅ Extracted structured resume saved to: resumes/Yeswanth_Yerra_CV_structured.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Personal Info\": {\n",
      "        \"name\": \"Yeswanth Yerra\",\n",
      "        \"emails\": [\n",
      "            \"yeswanthyerra07@gmail.com\"\n",
      "        ],\n",
      "        \"phones\": [\n",
      "            \"+91 9550413132\"\n",
      "        ],\n",
      "        \"links\": [],\n",
      "        \"raw\": \"['Yeswanth Yerra', '+91 9550413132 yeswanthyerra07@gmail.com Linkedin Github', 'Education', 'Pragati Engineering College', 'Surampalem, AP', 'Bachelor of Technology, Computer Science and Engineering (GPA- 8.14/10.0)', 'Aug. 2019 May 2023', 'Sasi junior College', 'Eluru, AP', 'Intermediate (GPA-9.8/10.0)']\"\n",
      "    },\n",
      "    \"Yeswanth Yerra\": \"+91 9550413132 yeswanthyerra07@gmail.com Linkedin Github\",\n",
      "    \"Education\": [\n",
      "        \"Pragati Engineering College\",\n",
      "        \"Surampalem, AP\",\n",
      "        \"Bachelor of Technology, Computer Science and Engineering (GPA- 8.14/10.0)\",\n",
      "        \"Aug. 2019 May 2023\",\n",
      "        \"Sasi junior College\",\n",
      "        \"Eluru, AP\",\n",
      "        \"Intermediate (GPA-9.8/10.0)\",\n",
      "        \"Aug 2017 May 2019\"\n",
      "    ],\n",
      "    \"Technical Skills\": [\n",
      "        \"Languages: Java, JavaScript, Python\",\n",
      "        \"Core Skills: Data Structures & Algorithms, Computer Networks , Operating Systems,DBMS\",\n",
      "        \"Frameworks: Spring Boot, Hibernate,React.js,Flask\",\n",
      "        \"Developer Tools: Git, Docker, Terraform, Ansible, Jenkins\",\n",
      "        \"Databases: MySQL, MongoDB, PostgreSQL\",\n",
      "        \"Cloud Computing: AWS,Kubernetes\"\n",
      "    ],\n",
      "    \"Experience\": [\n",
      "        \"Full Stack Development Intern\",\n",
      "        \"June 2024 - October 2024\",\n",
      "        \"Pantech Prolabs Pvt Ltd\",\n",
      "        \"Chennai,TN(Remote) Engineered a web application using Spring Boot for the backend, employing RESTful APIs and efficient database\",\n",
      "        \"querying with MongoDB. Gained expertise in design patterns and microservice architecture for building efficient, modular systems. Applied scalable web development practices to ensure optimal performance under increasing user loads and\",\n",
      "        \"Implemented authentication and authorization. Maintained high code quality by adhering to clean coding practices and leveraging testing frameworks like JUnit5\",\n",
      "        \"and Mockito, achieving 95% test coverage. Enhanced application reliability with Docker containerization, cutting deployment time by 50% and ensuring\",\n",
      "        \"consistent performance across all environments.\"\n",
      "    ],\n",
      "    \"Projects\": [\n",
      "        \"Movie Recommendation System Demo\",\n",
      "        \"October 2021 - March 2022 Led a team of 4 members for movie recommendation system by integrating collaborative filtering, content-based\",\n",
      "        \"filtering, and matrix factorization, resulting in a 33% improvement in accuracy using the MovieLens dataset. Extracted metadata features and employed TF-IDF for precise vector representation and enhancing feature quality\",\n",
      "        \"by 30%. Reduced RMSE from 2.2489 to 1.1000 by finetuning ALS matrix factorization for collaborative filtering. Addressed sparsity and cold-start challenges effectively using content-based filtering, reducing cold-start failures by\",\n",
      "        \"50% and improving recommendation coverage.\",\n",
      "        \"Network Intrusion Detection System (NIDS) Repo\",\n",
      "        \"October 2022 - March 2023 Engineered a Network Intrusion Detection System Utilizing BiLSTM to capture both forward and backward\",\n",
      "        \"dependencies,Integrated an attention layer to prioritize critical features, resulting in a 25% increase in detection\",\n",
      "        \"accuracy. Trained the model on the refined NSL-KDD dataset, addressing class imbalance and boosting overall performance\",\n",
      "        \"by 30%. Implemented hyperparameter tuning using grid search and random search, optimizing the BiLSTM networks\",\n",
      "        \"learning rate, batch size, and number of hidden layers, leading to a 20% increase in accuracy.\",\n",
      "        \"Job Application Microservices API Repo\",\n",
      "        \"October 2023 - December 2023 Designed and deployed a microservices architecture using Spring Boot, Eureka, and Feign clients, enabling seamless\",\n",
      "        \"communication between 3+ services, with 99.9% availability. Implemented JWT-based authentication and role-based security across 3 microservices, securing 100+ endpoints\",\n",
      "        \"with 2 or more roles per service. Integrated RabbitMQ for asynchronous communication, handling up to 50,000 messages per day between\",\n",
      "        \"microservices, improving system performance and reliability. Deployed the application on Kubernetes, leveraging auto-scaling and caching strategies to efficiently handle traffic\",\n",
      "        \"spikes and ensure high availability for up to 100,000 concurrent users.\",\n",
      "        \"Simple Song Search Repo\",\n",
      "        \"November 2024 -December 2024 Engineered a robust song search application using Flask and advanced algorithms like TF-IDF and cosine similarity\",\n",
      "        \"across a dataset of over 50,000 tracks Improving search speed and accuracy by 40% . Designed a responsive web interface and deployed the application on Render,achieving a 99.9% deployment success\",\n",
      "        \"rate.\"\n",
      "    ],\n",
      "    \"Certifications\": [\n",
      "        \"DataScience for Engineers - NPTEL-IIT MADRAS 2023\",\n",
      "        \"Software Engineering Job Simulation - Hewlett Packard Enterprise(Forage) 2024\",\n",
      "        \"GenAI path in Google Cloud Platform - Google\",\n",
      "        \"Java 17 MasterClass - Udemy\",\n",
      "        \"Machine Learning Using Python - SimpliLearn\",\n",
      "        \"Accounting and Data Analytics with Python - Coursera-Illinois University\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import unicodedata\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "# Logger Setup\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "\n",
    "# Regex Patterns\n",
    "\n",
    "EMAIL_RE = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n",
    "PHONE_RE = re.compile(r\"(\\+?\\d[\\d\\s\\-\\(\\)]{8,}\\d)\")\n",
    "LINK_RE = re.compile(\n",
    "    r\"(?:https?://)?(?:www\\.)?(?:linkedin|github|portfolio|medium|personal|behance)\\.[^\\s,]+\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "\n",
    "# Text Utilities\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = re.sub(r\"[^\\x20-\\x7E\\n]+\", \"\", text)\n",
    "    text = re.sub(r\"[•●–~►|#]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# Font Analysis\n",
    "\n",
    "def calculate_average_font_size(doc: fitz.Document) -> float:\n",
    "    total, count = 0, 0\n",
    "    for page in doc:\n",
    "        for block in page.get_text(\"dict\")[\"blocks\"]:\n",
    "            if block.get(\"type\") == 0:\n",
    "                for line in block.get(\"lines\", []):\n",
    "                    for span in line.get(\"spans\", []):\n",
    "                        total += span.get(\"size\", 0)\n",
    "                        count += 1\n",
    "    return total / count if count else 12\n",
    "\n",
    "\n",
    "# Name Extraction\n",
    "\n",
    "def extract_name_from_font(page: fitz.Page) -> str:\n",
    "    \"\"\"Detect candidate's name using largest font on the first page.\"\"\"\n",
    "    max_font = 0\n",
    "    name_candidate = \"\"\n",
    "    for block in page.get_text(\"dict\")[\"blocks\"]:\n",
    "        if block.get(\"type\") != 0:\n",
    "            continue\n",
    "        for line in block.get(\"lines\", []):\n",
    "            for span in line.get(\"spans\", []):\n",
    "                if span[\"size\"] > max_font:\n",
    "                    max_font = span[\"size\"]\n",
    "                    name_candidate = span[\"text\"].strip()\n",
    "    logging.info(f\"🧠 Name candidate (largest font): {name_candidate}\")\n",
    "    return name_candidate\n",
    "\n",
    "\n",
    "# Contact Info Extraction\n",
    "\n",
    "def extract_contact_info(text: str) -> Dict:\n",
    "    emails = EMAIL_RE.findall(text)\n",
    "    raw_phones = [m.group(0).strip() for m in PHONE_RE.finditer(text)]\n",
    "    links = LINK_RE.findall(text)\n",
    "\n",
    "    phones = []\n",
    "    for p in raw_phones:\n",
    "        digits = re.sub(r\"\\D\", \"\", p)\n",
    "        if len(digits) < 9 or len(digits) > 15:\n",
    "            continue\n",
    "        if re.match(r\"20\\d{2}\", digits[:4]):  # avoid years\n",
    "            continue\n",
    "        phones.append(p.strip())\n",
    "\n",
    "    return {\n",
    "        \"emails\": list(set(emails)),\n",
    "        \"phones\": list(set(phones)),\n",
    "        \"links\": list(set(links)),\n",
    "    }\n",
    "\n",
    "\n",
    "# Section Extraction\n",
    "\n",
    "def extract_sections_from_resume(pdf_path: str, headings: List[str] = None) -> Dict:\n",
    "    if headings is None:\n",
    "        headings = [\n",
    "            r\"Objective\", r\"Summary\", r\"Education\", r\"Experience\", r\"Work Experience\",\n",
    "            r\"Professional Experience\", r\"Projects\", r\"Skills\", r\"Technical Skills\",\n",
    "            r\"Certifications\", r\"Internship\", r\"Achievements\", r\"Hobbies\", r\"Interests\"\n",
    "        ]\n",
    "\n",
    "    heading_pattern = re.compile(r\"^\\s*(\" + r\"|\".join(headings) + r\")\\s*$\", re.IGNORECASE)\n",
    "\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\" Failed to open PDF: {e}\")\n",
    "        return {}\n",
    "\n",
    "    avg_font = calculate_average_font_size(doc)\n",
    "    logging.info(f\" Average font size: {avg_font:.2f}\")\n",
    "\n",
    "    sections = {}\n",
    "    current_heading = None\n",
    "    current_text = []\n",
    "\n",
    "    for page in doc:\n",
    "        for block in page.get_text(\"dict\")[\"blocks\"]:\n",
    "            if block.get(\"type\") != 0:\n",
    "                continue\n",
    "            for line in block.get(\"lines\", []):\n",
    "                line_text = \"\".join(span.get(\"text\", \"\") for span in line.get(\"spans\", [])).strip()\n",
    "                if not line_text:\n",
    "                    continue\n",
    "\n",
    "                max_font = max((span.get(\"size\", 0) for span in line.get(\"spans\", [])), default=0)\n",
    "                is_heading = bool(heading_pattern.match(line_text)) or max_font > 1.5 * avg_font\n",
    "\n",
    "                if is_heading:\n",
    "                    if current_heading:\n",
    "                        sections[current_heading] = clean_text(\"\\n\".join(current_text))\n",
    "                    current_heading = line_text.strip().title()\n",
    "                    current_text = []\n",
    "                else:\n",
    "                    if current_heading:\n",
    "                        current_text.append(line_text)\n",
    "\n",
    "    if current_heading:\n",
    "        sections[current_heading] = clean_text(\"\\n\".join(current_text))\n",
    "\n",
    "    return sections\n",
    "\n",
    "\n",
    "# Combine All Logic\n",
    "\n",
    "def extract_resume_data(pdf_path: str) -> Dict:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    first_page = doc[0]\n",
    "\n",
    "    first_page_text = first_page.get_text(\"text\")\n",
    "    name = extract_name_from_font(first_page)\n",
    "    contact_info = extract_contact_info(first_page_text)\n",
    "\n",
    "    personal_info = {\n",
    "        \"name\": name,\n",
    "        \"emails\": contact_info.get(\"emails\", []),\n",
    "        \"phones\": contact_info.get(\"phones\", []),\n",
    "        \"links\": contact_info.get(\"links\", []),\n",
    "        \"raw\": clean_text(str(first_page_text.split(\"\\n\")[0:10]))\n",
    "    }\n",
    "\n",
    "    sections = extract_sections_from_resume(pdf_path)\n",
    "\n",
    "    result = {\"Personal Info\": personal_info}\n",
    "    result.update(sections)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Split Multiline Sections (No Regex)\n",
    "\n",
    "def split_multiline_sections(data: dict) -> dict:\n",
    "    \"\"\"Convert multiline strings into list items safely (no regex).\"\"\"\n",
    "    for key, value in list(data.items()):\n",
    "        if isinstance(value, dict):\n",
    "            data[key] = split_multiline_sections(value)\n",
    "        elif isinstance(value, str) and \"\\n\" in value:\n",
    "            lines = [line.strip() for line in value.split(\"\\n\") if line.strip()]\n",
    "            data[key] = lines\n",
    "    return data\n",
    "\n",
    "\n",
    "# Save to JSON\n",
    "\n",
    "def save_to_json(pdf_path: str, data: Dict):\n",
    "    # --- Modification Start ---\n",
    "    # Add the hardcoded key-value pairs to the data dictionary\n",
    "    data[\"preferred_title\"] = \"Machine Learning Engineer\"\n",
    "    data[\"preferred_job_location\"] = \"India\"\n",
    "    # --- Modification End ---\n",
    "\n",
    "    output_path = pdf_path.replace(\".pdf\", \"_structured.json\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "    logging.info(f\"✅ Extracted structured resume saved to: {output_path}\")\n",
    "\n",
    "\n",
    "# Run Example\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"resumes/Yeswanth_Yerra_CV.pdf\"  # Change path to your PDF\n",
    "    data = extract_resume_data(pdf_path)\n",
    "    data = split_multiline_sections(data)\n",
    "    print(json.dumps(data, indent=4, ensure_ascii=False))\n",
    "    save_to_json(pdf_path, data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
