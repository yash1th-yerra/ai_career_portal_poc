{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f81fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import json\n",
    "import os\n",
    "# Set your API key (assumes you have a service account key JSON or API key)\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if api_key:\n",
    "    genai.configure(api_key=api_key)\n",
    "    print(\"Gemini API key configured successfully from environment variable.\")\n",
    "else:\n",
    "    print(\"Error: GEMINI_API_KEY environment variable not found.\")\n",
    "    # You might want to raise an exception or exit the script here\n",
    "    # raise ValueError(\"GE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aba7bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gemini(prompt, model=\"gemini-2.5-flash\", temperature=0):\n",
    "    \"\"\"\n",
    "    Call Gemini 2.5 Flash for a single-turn generative response.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel(model_name=model)\n",
    "    response = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config=genai.types.GenerationConfig(temperature=temperature)\n",
    "    )\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a225d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import time \n",
    "\n",
    "import json\n",
    "\n",
    "def generate_field_answers(fields, resume_json):\n",
    "    answers = []\n",
    "    for field in fields:\n",
    "        question = field[\"field_name\"]\n",
    "        options_text = \"; \".join(field[\"options\"]) if field[\"options\"] else \"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are an expert LinkedIn Easy Apply assistant that auto-fills form fields intelligently.\n",
    "\n",
    "üéØ **Your goal**: Provide the most accurate and recruiter-friendly answer for each field using the candidate‚Äôs resume data.\n",
    "\n",
    "üß† **Input Context**\n",
    "- Resume JSON: {json.dumps(resume_json)}\n",
    "- Field question: {question}\n",
    "- Field type: {field['field_type']}\n",
    "- Current value: {field['value']}\n",
    "- Options (if any): {options_text}\n",
    "\n",
    "---\n",
    "\n",
    "### üß© **Core Principles**\n",
    "1Ô∏è‚É£ Favor the candidate by highlighting skills, experience, and achievements.  \n",
    "2Ô∏è‚É£ Favor the recruiter by ensuring the response looks relevant, confident, and professional.  \n",
    "3Ô∏è‚É£ Always produce an answer that can be directly inserted into a form field ‚Äî no commentary, no extra text.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† **Intelligent Behavior Rules**\n",
    "\n",
    "#### üìä **1. Numerical / Experience-based questions**\n",
    "- If the question asks:\n",
    "  - ‚ÄúHow many years of experience‚Äù, ‚ÄúExperience in‚Äù, or similar ‚Üí output ONLY a number.\n",
    "  - Example: \"Experience in Gen-AI?\" ‚Üí `2`\n",
    "  - Example: \"How many years of experience in Python?\" ‚Üí `3`\n",
    "- Use fractional years:\n",
    "  - 2 years 3 months ‚Üí 2\n",
    "  - 2 years 8 months ‚Üí 3\n",
    "- Don‚Äôt include words like ‚Äúyears‚Äù, ‚Äúyrs‚Äù, or ‚Äúmonths‚Äù ‚Äî just the number.\n",
    "\n",
    "#### üí∞ **2. Salary or compensation**\n",
    "- If question involves salary, CTC, or pay expectations ‚Üí return a **range** like:\n",
    "  - `100000 - 200000`\n",
    "- Use realistic and market-aligned numbers based on resume experience.\n",
    "\n",
    "#### üí¨ **3. Text / Paragraph fields**\n",
    "- If question expects a descriptive answer (like ‚ÄúWhy should we hire you?‚Äù or ‚ÄúTell us about yourself‚Äù):\n",
    "  - Write 1‚Äì3 sentences that sound professional and natural.\n",
    "  - Use resume highlights (skills, projects, experience) to make it personal.\n",
    "  - Example: \"I'm a software engineer with strong expertise in Gen-AI, automation, and large-scale system design.\"\n",
    "\n",
    "#### üéØ **4. Select / Multi-select / Radio / Checkbox**\n",
    "- Select the **most relevant option(s)** aligned with the candidate‚Äôs skills, job role, or experience.\n",
    "- Return **only** the selected option(s) text, not an explanation.\n",
    "\n",
    "#### üßæ **5. Missing or unclear info**\n",
    "- If resume doesn‚Äôt explicitly provide the answer:\n",
    "  - Infer a realistic, professional value.\n",
    "  - Avoid placeholders like ‚ÄúN/A‚Äù, ‚Äúsample‚Äù, ‚Äúnot applicable‚Äù.\n",
    "\n",
    "#### ‚ö° **6. Strict output format**\n",
    "- Return only the final value ‚Äî no explanations, quotes, or formatting.\n",
    "- The answer must be:\n",
    "  - a number ‚Üí for numeric questions\n",
    "  - a text phrase ‚Üí for open-ended questions\n",
    "  - a valid option ‚Üí for dropdowns/radio/multi-select\n",
    "  - a salary range ‚Üí for pay-related fields\n",
    "\n",
    "---\n",
    "\n",
    "### üß© **Output Expectation**\n",
    "Return ONLY the answer text (no markdown, no commentary).\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            answer = ask_gemini(prompt)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating answer for field '{question}': {e}\")\n",
    "            answer = \"\"\n",
    "        \n",
    "        field_copy = field.copy()\n",
    "        field_copy[\"generated_answer\"] = answer.strip()\n",
    "        answers.append(field_copy)\n",
    "        time.sleep(0.5)  # rate-limit Gemini calls lightly\n",
    "    return answers\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Save generated answers CSV\n",
    "# -----------------------------\n",
    "def save_answers_to_csv(fields_with_answers, filename=\"easy_apply_answers.csv\"):\n",
    "    file_exists = os.path.exists(filename)\n",
    "    with open(filename, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"field_name\", \"field_type\", \"value\", \"options\", \"generated_answer\"])\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        for row in fields_with_answers:\n",
    "            row_copy = row.copy()\n",
    "            row_copy.pop(\"element\", None)\n",
    "            row_copy[\"options\"] = \"; \".join(row_copy.get(\"options\", [])) if row_copy.get(\"options\") else \"\"\n",
    "            writer.writerow(row_copy)\n",
    "    print(f\"‚úÖ Answers appended to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6490d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ==== Country domain mapping ====\n",
    "indeed_domains = {\n",
    "    \"Argentina\": \"ar\", \"Australia\": \"au\", \"Austria\": \"at\", \"Bahrain\": \"bh\", \"Belgium\": \"be\",\n",
    "    \"Brazil\": \"br\", \"Canada\": \"ca\", \"Chile\": \"cl\", \"China\": \"cn\", \"Colombia\": \"co\",\n",
    "    \"Costa Rica\": \"cr\", \"Czech Republic\": \"cz\", \"Denmark\": \"dk\", \"Ecuador\": \"ec\", \"Egypt\": \"eg\",\n",
    "    \"Finland\": \"fi\", \"France\": \"fr\", \"Germany\": \"de\", \"Greece\": \"gr\", \"Hong Kong\": \"hk\",\n",
    "    \"Hungary\": \"hu\", \"India\": \"in\", \"Indonesia\": \"id\", \"Ireland\": \"ie\", \"Israel\": \"il\",\n",
    "    \"Italy\": \"it\", \"Japan\": \"jp\", \"Kuwait\": \"kw\", \"Luxembourg\": \"lu\", \"Malaysia\": \"my\",\n",
    "    \"Mexico\": \"mx\", \"Morocco\": \"ma\", \"Netherlands\": \"nl\", \"New Zealand\": \"nz\", \"Nigeria\": \"ng\",\n",
    "    \"Norway\": \"no\", \"Oman\": \"om\", \"Pakistan\": \"pk\", \"Panama\": \"pa\", \"Peru\": \"pe\",\n",
    "    \"Philippines\": \"ph\", \"Poland\": \"pl\", \"Portugal\": \"pt\", \"Qatar\": \"qa\", \"Romania\": \"ro\",\n",
    "    \"Saudi Arabia\": \"sa\", \"Singapore\": \"sg\", \"South Africa\": \"za\", \"South Korea\": \"kr\",\n",
    "    \"Spain\": \"es\", \"Sweden\": \"se\", \"Switzerland\": \"ch\", \"Taiwan\": \"tw\", \"Thailand\": \"th\",\n",
    "    \"Turkey\": \"tr\", \"Ukraine\": \"ua\", \"United Arab Emirates\": \"ae\", \"United Kingdom\": \"uk\",\n",
    "    \"United States\": \"\", \"Uruguay\": \"uy\", \"Venezuela\": \"ve\", \"Vietnam\": \"vn\"\n",
    "}\n",
    "\n",
    "# ==== Build Indeed URL ====\n",
    "def build_indeed_url(job_title, location, country=\"India\", start=0):\n",
    "    country_code = indeed_domains.get(country, \"\")\n",
    "    base_url = f\"https://{country_code}.indeed.com/jobs\" if country_code else \"https://www.indeed.com/jobs\"\n",
    "    job_title_encoded = job_title.replace(\" \", \"+\")\n",
    "    location_encoded = location.replace(\" \", \"+\")\n",
    "    return f\"{base_url}?q={job_title_encoded}&l={location_encoded}&from=searchOnDesktopSerp&start={start}\"\n",
    "\n",
    "# ==== Load preferred title and location from JSON ====\n",
    "def get_preferred_job_and_location(json_path=\"resumes/Yeswanth_Yerra_CV_structured.json\"):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    preferred_title = data.get(\"preferred_title\", \"Software Engineer\")\n",
    "    preferred_location = data.get(\"preferred_location\", \"India\")\n",
    "    return preferred_title, preferred_location\n",
    "\n",
    "# ==== Modular scraper function ====\n",
    "def scrape_indeed_jobs(job_title=None, location=None, country=\"India\", max_pages=3, save_csv=True, output_file=\"indeed_jobs.csv\"):\n",
    "    # If not provided, get from JSON\n",
    "    if not job_title or not location:\n",
    "        job_title, location = get_preferred_job_and_location()\n",
    "\n",
    "    print(f\"üîç Scraping Indeed for '{job_title}' jobs in '{location}', Country: {country}\")\n",
    "\n",
    "    # ==== Setup Chrome driver ====\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "    chrome_options.add_argument(\"--disable-notifications\")\n",
    "    chrome_options.add_argument(\"--headless=new\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    jobs = []\n",
    "\n",
    "    for page in range(max_pages):\n",
    "        start = page * 10\n",
    "        search_url = build_indeed_url(job_title, location, country, start)\n",
    "        print(f\"\\nDEBUG: Processing page {page+1} | URL: {search_url}\")\n",
    "\n",
    "        driver.get(search_url)\n",
    "        time.sleep(3)\n",
    "\n",
    "        # ==== Wait for job cards ====\n",
    "        try:\n",
    "            job_cards = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.job_seen_beacon\"))\n",
    "            )\n",
    "            print(f\"DEBUG: Found {len(job_cards)} job cards on page {page+1}\")\n",
    "        except:\n",
    "            print(f\"No job cards found on page {page+1}, stopping.\")\n",
    "            break\n",
    "\n",
    "        for idx, card in enumerate(job_cards, start=1):\n",
    "            print(f\"\\nDEBUG: Processing job card #{idx} on page {page+1}\")\n",
    "            \n",
    "            # Title\n",
    "            try:\n",
    "                title_elem = card.find_element(By.CSS_SELECTOR, \"h2 > a\")\n",
    "                title = title_elem.text\n",
    "                job_link = title_elem.get_attribute(\"href\")\n",
    "            except:\n",
    "                title = \"N/A\"\n",
    "                job_link = None\n",
    "            \n",
    "            # Company\n",
    "            try:\n",
    "                company_elem = card.find_element(By.CSS_SELECTOR, \"span[data-testid='company-name']\")\n",
    "                company = company_elem.text\n",
    "            except:\n",
    "                company = \"N/A\"\n",
    "            \n",
    "            # Location\n",
    "            try:\n",
    "                location_elem = card.find_element(By.CSS_SELECTOR, \"div[data-testid='text-location']\")\n",
    "                location_text = location_elem.text\n",
    "            except:\n",
    "                location_text = \"N/A\"\n",
    "            \n",
    "            # ==== Click to load job description in right pane ====\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", title_elem)\n",
    "                title_elem.click()\n",
    "                job_desc_elem = WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_element_located((By.ID, \"jobDescriptionText\"))\n",
    "                )\n",
    "                job_description = job_desc_elem.text\n",
    "            except:\n",
    "                job_description = \"N/A\"\n",
    "\n",
    "            # ==== Detect Apply Type and Apply Link ====\n",
    "            try:\n",
    "                apply_button = driver.find_element(By.CSS_SELECTOR, \"span.indeed-apply-status-not-applied button\")\n",
    "                apply_type = \"Apply Now\"\n",
    "                apply_link = apply_button.get_attribute(\"onclick\") or job_link\n",
    "            except:\n",
    "                apply_type = \"Apply on Company Site\"\n",
    "                apply_link = job_link\n",
    "\n",
    "            jobs.append({\n",
    "                \"Title\": title,\n",
    "                \"Company\": company,\n",
    "                \"Location\": location_text,\n",
    "                \"Description\": job_description,\n",
    "                \"Apply Type\": apply_type,\n",
    "                \"Apply Link\": apply_link\n",
    "            })\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # ==== Save to CSV ====\n",
    "    if save_csv and jobs:\n",
    "        df = pd.DataFrame(jobs)\n",
    "        df.replace(\"N/A\", pd.NA, inplace=True)\n",
    "        df.dropna(subset=[\"Title\", \"Company\", \"Location\", \"Description\"], inplace=True)\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"\\n‚úÖ Scraping complete. Saved to {output_file}\")\n",
    "\n",
    "    return jobs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85fece6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import logging\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "class LinkedInJobScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        self.headless = headless\n",
    "        self.driver = None\n",
    "        self.setup_driver()\n",
    "\n",
    "    def setup_driver(self):\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"--start-maximized\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "        options.add_argument(\"--disable-popup-blocking\")\n",
    "        options.add_argument(\"--disable-notifications\")\n",
    "        options.add_argument(\"--disable-infobars\")\n",
    "        options.add_argument(\n",
    "            \"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        )\n",
    "        if self.headless:\n",
    "            options.add_argument(\"--headless=new\")\n",
    "\n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "        self.driver.execute_script(\n",
    "            \"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\"\n",
    "        )\n",
    "        logging.info(\"Chrome WebDriver initialized successfully\")\n",
    "\n",
    "    # ---------------------------\n",
    "    def _get_time_filter(self, days: int) -> str:\n",
    "        return {1: \"r86400\", 3: \"r259200\", 7: \"r604800\", 30: \"r2592000\"}.get(days, \"r604800\")\n",
    "\n",
    "    def build_search_url(self, job_title: str, location: str, experience_level=None, time_posted=None):\n",
    "        base_url = \"https://www.linkedin.com/jobs/search/\"\n",
    "        params = {\n",
    "            \"keywords\": urllib.parse.quote(job_title),\n",
    "            \"location\": urllib.parse.quote(location),\n",
    "            \"f_TPR\": time_posted\n",
    "        }\n",
    "        query = \"&\".join([f\"{k}={v}\" for k, v in params.items() if v])\n",
    "        return f\"{base_url}?{query}\"\n",
    "\n",
    "    # ---------------------------\n",
    "    def scrape_job_details(self, job_url: str):\n",
    "        \"\"\"\n",
    "        Open a job detail page and extract:\n",
    "        - Apply type (Easy Apply or Apply)\n",
    "        - Employment type\n",
    "        - Job description\n",
    "        \"\"\"\n",
    "        self.driver.get(job_url)\n",
    "        time.sleep(3)  # Let the page load\n",
    "\n",
    "        soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Apply type detection\n",
    "        # Wait for apply button or code tag dynamically\n",
    "        apply_type = \"Unknown\"\n",
    "        try:\n",
    "            WebDriverWait(self.driver, 5).until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (By.CSS_SELECTOR, \"button.top-card-layout__cta--primary, code#applyUrl\")\n",
    "                )\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "\n",
    "        # First, check code tag\n",
    "        code_tag = soup.find(\"code\", id=\"applyUrl\")\n",
    "        if code_tag and code_tag.text.strip():\n",
    "            apply_type = \"Apply (Offsite)\"\n",
    "        else:\n",
    "            # Then check button attribute\n",
    "            apply_button = soup.find(\"button\", class_=\"top-card-layout__cta--primary\")\n",
    "            if apply_button:\n",
    "                dt_name = apply_button.get(\"data-tracking-control-name\", \"\").lower()\n",
    "                if \"offsite\" in dt_name:\n",
    "                    apply_type = \"Apply (Offsite)\"\n",
    "                elif \"onsite\" in dt_name:\n",
    "                    apply_type = \"Easy Apply\"\n",
    "\n",
    "\n",
    "        # Employment type\n",
    "        employment_type = None\n",
    "        criteria_items = soup.select(\"ul.description__job-criteria-list li.description__job-criteria-item\")\n",
    "        for item in criteria_items:\n",
    "            header = item.find(\"h3\", class_=\"description__job-criteria-subheader\")\n",
    "            if header and header.text.strip() == \"Employment type\":\n",
    "                span = item.find(\"span\", class_=\"description__job-criteria-text--criteria\")\n",
    "                if span:\n",
    "                    employment_type = span.text.strip()\n",
    "                break\n",
    "\n",
    "        # Job description\n",
    "        job_description = None\n",
    "        desc_div = soup.find(\"div\", class_=\"description__text--rich\")\n",
    "        if desc_div:\n",
    "            markup_div = desc_div.find(\"div\", class_=\"show-more-less-html__markup\")\n",
    "            if markup_div:\n",
    "                job_description = markup_div.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "        return {\n",
    "            \"apply_link\": job_url,  # keep job url as fallback\n",
    "            \"apply_type\": apply_type,\n",
    "            \"employment_type\": employment_type,\n",
    "            \"job_description\": job_description\n",
    "        }\n",
    "\n",
    "    # ---------------------------\n",
    "    def scrape_jobs(self, job_title: str, location: str = \"India\", pages: int = 1,\n",
    "                    experience_level: str = None, days_back: int = 7,\n",
    "                    csv_filename: str = \"csv/linkedin_jobs.csv\"):\n",
    "\n",
    "        time_filter = self._get_time_filter(days_back)\n",
    "        search_url = self.build_search_url(job_title, location, experience_level, time_filter)\n",
    "        self.driver.get(search_url)\n",
    "        time.sleep(4)\n",
    "\n",
    "        logging.info(f\"Scraping '{job_title}' jobs in {location} (pages={pages})\")\n",
    "\n",
    "        # Scroll and click \"Show more\" to load more jobs\n",
    "        for _ in range(pages):\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(random.uniform(3, 5))\n",
    "            try:\n",
    "                btn = WebDriverWait(self.driver, 3).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, \"//button[contains(., 'Show more')]\"))\n",
    "                )\n",
    "                btn.click()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # Parse loaded page\n",
    "        soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "        job_cards = soup.find_all(\"div\", class_=\"base-card\")\n",
    "        logging.info(f\"Found {len(job_cards)} job cards\")\n",
    "\n",
    "        jobs_data = []\n",
    "        # job_cards = job_cards[:3]\n",
    "        # print(\"scraping only 3 jobs\")\n",
    "        for card in job_cards:\n",
    "            try:\n",
    "                title = card.find(\"h3\", class_=\"base-search-card__title\").get_text(strip=True)\n",
    "                company = card.find(\"h4\", class_=\"base-search-card__subtitle\").get_text(strip=True)\n",
    "                location_text = card.find(\"span\", class_=\"job-search-card__location\").get_text(strip=True)\n",
    "                job_url_elem = card.find(\"a\", class_=\"base-card__full-link\")\n",
    "                if not job_url_elem:\n",
    "                    continue\n",
    "                job_url = job_url_elem.get(\"href\")\n",
    "\n",
    "                # Scrape details from job page\n",
    "                details = self.scrape_job_details(job_url)\n",
    "\n",
    "                job_data = {\n",
    "                    \"title\": title,\n",
    "                    \"company\": company,\n",
    "                    \"location\": location_text,\n",
    "                    **details\n",
    "                }\n",
    "                jobs_data.append(job_data)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Skipping a job due to error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Save to CSV\n",
    "        if jobs_data:\n",
    "            keys = jobs_data[0].keys()\n",
    "            with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                dict_writer = csv.DictWriter(f, fieldnames=keys)\n",
    "                dict_writer.writeheader()\n",
    "                dict_writer.writerows(jobs_data)\n",
    "            logging.info(f\"Saved {len(jobs_data)} jobs to {csv_filename}\")\n",
    "\n",
    "        return jobs_data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe5b18e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import unicodedata\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "# Logger Setup\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "\n",
    "# Regex Patterns\n",
    "\n",
    "EMAIL_RE = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n",
    "PHONE_RE = re.compile(r\"(\\+?\\d[\\d\\s\\-\\(\\)]{8,}\\d)\")\n",
    "LINK_RE = re.compile(\n",
    "    r\"(?:https?://)?(?:www\\.)?(?:linkedin|github|portfolio|medium|personal|behance)\\.[^\\s,]+\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "\n",
    "# Text Utilities\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = re.sub(r\"[^\\x20-\\x7E\\n]+\", \"\", text)\n",
    "    text = re.sub(r\"[‚Ä¢‚óè‚Äì~‚ñ∫|#]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# Font Analysis\n",
    "\n",
    "def calculate_average_font_size(doc: fitz.Document) -> float:\n",
    "    total, count = 0, 0\n",
    "    for page in doc:\n",
    "        for block in page.get_text(\"dict\")[\"blocks\"]:\n",
    "            if block.get(\"type\") == 0:\n",
    "                for line in block.get(\"lines\", []):\n",
    "                    for span in line.get(\"spans\", []):\n",
    "                        total += span.get(\"size\", 0)\n",
    "                        count += 1\n",
    "    return total / count if count else 12\n",
    "\n",
    "\n",
    "# Name Extraction\n",
    "\n",
    "def extract_name_from_font(page: fitz.Page) -> str:\n",
    "    \"\"\"Detect candidate's name using largest font on the first page.\"\"\"\n",
    "    max_font = 0\n",
    "    name_candidate = \"\"\n",
    "    for block in page.get_text(\"dict\")[\"blocks\"]:\n",
    "        if block.get(\"type\") != 0:\n",
    "            continue\n",
    "        for line in block.get(\"lines\", []):\n",
    "            for span in line.get(\"spans\", []):\n",
    "                if span[\"size\"] > max_font:\n",
    "                    max_font = span[\"size\"]\n",
    "                    name_candidate = span[\"text\"].strip()\n",
    "    logging.info(f\"üß† Name candidate (largest font): {name_candidate}\")\n",
    "    return name_candidate\n",
    "\n",
    "\n",
    "# Contact Info Extraction\n",
    "\n",
    "def extract_contact_info(text: str) -> Dict:\n",
    "    emails = EMAIL_RE.findall(text)\n",
    "    raw_phones = [m.group(0).strip() for m in PHONE_RE.finditer(text)]\n",
    "    links = LINK_RE.findall(text)\n",
    "\n",
    "    phones = []\n",
    "    for p in raw_phones:\n",
    "        digits = re.sub(r\"\\D\", \"\", p)\n",
    "        if len(digits) < 9 or len(digits) > 15:\n",
    "            continue\n",
    "        if re.match(r\"20\\d{2}\", digits[:4]):  # avoid years\n",
    "            continue\n",
    "        phones.append(p.strip())\n",
    "\n",
    "    return {\n",
    "        \"emails\": list(set(emails)),\n",
    "        \"phones\": list(set(phones)),\n",
    "        \"links\": list(set(links)),\n",
    "    }\n",
    "\n",
    "\n",
    "# Section Extraction\n",
    "\n",
    "def extract_sections_from_resume(pdf_path: str, headings: List[str] = None) -> Dict:\n",
    "    if headings is None:\n",
    "        headings = [\n",
    "            r\"Objective\", r\"Summary\", r\"Education\", r\"Experience\", r\"Work Experience\",\n",
    "            r\"Professional Experience\", r\"Projects\", r\"Skills\", r\"Technical Skills\",\n",
    "            r\"Certifications\", r\"Internship\", r\"Achievements\", r\"Hobbies\", r\"Interests\"\n",
    "        ]\n",
    "\n",
    "    heading_pattern = re.compile(r\"^\\s*(\" + r\"|\".join(headings) + r\")\\s*$\", re.IGNORECASE)\n",
    "\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\" Failed to open PDF: {e}\")\n",
    "        return {}\n",
    "\n",
    "    avg_font = calculate_average_font_size(doc)\n",
    "    logging.info(f\" Average font size: {avg_font:.2f}\")\n",
    "\n",
    "    sections = {}\n",
    "    current_heading = None\n",
    "    current_text = []\n",
    "\n",
    "    for page in doc:\n",
    "        for block in page.get_text(\"dict\")[\"blocks\"]:\n",
    "            if block.get(\"type\") != 0:\n",
    "                continue\n",
    "            for line in block.get(\"lines\", []):\n",
    "                line_text = \"\".join(span.get(\"text\", \"\") for span in line.get(\"spans\", [])).strip()\n",
    "                if not line_text:\n",
    "                    continue\n",
    "\n",
    "                max_font = max((span.get(\"size\", 0) for span in line.get(\"spans\", [])), default=0)\n",
    "                is_heading = bool(heading_pattern.match(line_text)) or max_font > 1.5 * avg_font\n",
    "\n",
    "                if is_heading:\n",
    "                    if current_heading:\n",
    "                        sections[current_heading] = clean_text(\"\\n\".join(current_text))\n",
    "                    current_heading = line_text.strip().title()\n",
    "                    current_text = []\n",
    "                else:\n",
    "                    if current_heading:\n",
    "                        current_text.append(line_text)\n",
    "\n",
    "    if current_heading:\n",
    "        sections[current_heading] = clean_text(\"\\n\".join(current_text))\n",
    "\n",
    "    return sections\n",
    "\n",
    "\n",
    "# Combine All Logic\n",
    "\n",
    "def extract_resume_data(pdf_path: str) -> Dict:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    first_page = doc[0]\n",
    "\n",
    "    first_page_text = first_page.get_text(\"text\")\n",
    "    name = extract_name_from_font(first_page)\n",
    "    contact_info = extract_contact_info(first_page_text)\n",
    "\n",
    "    personal_info = {\n",
    "        \"name\": name,\n",
    "        \"emails\": contact_info.get(\"emails\", []),\n",
    "        \"phones\": contact_info.get(\"phones\", []),\n",
    "        \"links\": contact_info.get(\"links\", []),\n",
    "        \"raw\": clean_text(str(first_page_text.split(\"\\n\")[0:10]))\n",
    "    }\n",
    "\n",
    "    sections = extract_sections_from_resume(pdf_path)\n",
    "\n",
    "    result = {\"Personal Info\": personal_info}\n",
    "    result.update(sections)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Split Multiline Sections (No Regex)\n",
    "\n",
    "def split_multiline_sections(data: dict) -> dict:\n",
    "    \"\"\"Convert multiline strings into list items safely (no regex).\"\"\"\n",
    "    for key, value in list(data.items()):\n",
    "        if isinstance(value, dict):\n",
    "            data[key] = split_multiline_sections(value)\n",
    "        elif isinstance(value, str) and \"\\n\" in value:\n",
    "            lines = [line.strip() for line in value.split(\"\\n\") if line.strip()]\n",
    "            data[key] = lines\n",
    "    return data\n",
    "\n",
    "\n",
    "# Save to JSON\n",
    "\n",
    "def save_to_json(pdf_path: str, data: Dict):\n",
    "    # --- Modification Start ---\n",
    "    # Add the hardcoded key-value pairs to the data dictionary\n",
    "    data[\"preferred_title\"] = \"Machine Learning Engineer\"\n",
    "    data[\"preferred_job_location\"] = \"India\"\n",
    "    # --- Modification End ---\n",
    "\n",
    "    output_path = pdf_path.replace(\".pdf\", \"_structured.json\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "    logging.info(f\"‚úÖ Extracted structured resume saved to: {output_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0cca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_easy_apply_jobs(csv_path, resume_json_path, resume_drive_url):\n",
    "    import os\n",
    "    import time\n",
    "    import csv\n",
    "    import pickle\n",
    "    import json\n",
    "    import requests\n",
    "    import tempfile\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.common.keys import Keys\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.common.exceptions import NoSuchElementException\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "    # -----------------------------\n",
    "    # Configuration\n",
    "    # -----------------------------\n",
    "    LINKEDIN_EMAIL = os.getenv(\"LINKEDIN_EMAIL\")\n",
    "    LINKEDIN_PASSWORD = os.getenv(\"LINKEDIN_PASSWORD\")\n",
    "    COOKIE_FILE = \"linkedin_cookies.pkl\"\n",
    "    CSV_PATH = csv_path\n",
    "    RESUME_DRIVE_URL = resume_drive_url\n",
    "\n",
    "    # -----------------------------\n",
    "    # Launch Chrome\n",
    "    # -----------------------------\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Login functions\n",
    "    # -----------------------------\n",
    "    def try_login_with_cookies():\n",
    "        if os.path.exists(COOKIE_FILE):\n",
    "            driver.get(\"https://www.linkedin.com\")\n",
    "            with open(COOKIE_FILE, \"rb\") as f:\n",
    "                cookies = pickle.load(f)\n",
    "                for cookie in cookies:\n",
    "                    driver.add_cookie(cookie)\n",
    "            driver.refresh()\n",
    "            time.sleep(3)\n",
    "            try:\n",
    "                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"img.global-nav__me-photo\")))\n",
    "                print(\"‚úÖ Logged in using cookies\")\n",
    "                return True\n",
    "            except:\n",
    "                print(\"‚ùå Cookies expired or invalid\")\n",
    "                return False\n",
    "        return False\n",
    "\n",
    "    def login_with_credentials():\n",
    "        driver.get(\"https://www.linkedin.com/login\")\n",
    "        username_input = wait.until(EC.presence_of_element_located((By.ID, \"username\")))\n",
    "        username_input.send_keys(LINKEDIN_EMAIL)\n",
    "        password_input = driver.find_element(By.ID, \"password\")\n",
    "        password_input.send_keys(LINKEDIN_PASSWORD)\n",
    "        password_input.send_keys(Keys.RETURN)\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"img.global-nav__me-photo\")))\n",
    "        with open(COOKIE_FILE, \"wb\") as f:\n",
    "            pickle.dump(driver.get_cookies(), f)\n",
    "        print(\"‚úÖ Logged in and cookies saved\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Resume download with '-drive'\n",
    "    # -----------------------------\n",
    "    def download_resume_from_drive(drive_url):\n",
    "        file_id = drive_url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "        download_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
    "        resp = requests.get(download_url, stream=True)\n",
    "        if resp.status_code != 200:\n",
    "            raise Exception(f\"Failed to download resume, status code {resp.status_code}\")\n",
    "\n",
    "        cd = resp.headers.get(\"content-disposition\", \"\")\n",
    "        if \"filename=\" in cd:\n",
    "            original_name = cd.split(\"filename=\")[1].strip('\"')\n",
    "        else:\n",
    "            original_name = f\"resume_{file_id}.pdf\"\n",
    "\n",
    "        base, ext = os.path.splitext(original_name)\n",
    "        new_name = f\"{base}-drive{ext}\"\n",
    "        temp_dir = tempfile.gettempdir()\n",
    "        temp_path = os.path.join(temp_dir, new_name)\n",
    "\n",
    "        with open(temp_path, \"wb\") as f:\n",
    "            for chunk in resp.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "        return temp_path\n",
    "\n",
    "    # -----------------------------\n",
    "    # Easy Apply click\n",
    "    # -----------------------------\n",
    "    def click_easy_apply():\n",
    "        try:\n",
    "            easy_apply_button = driver.find_element(By.CSS_SELECTOR, \"button.jobs-apply-button\")\n",
    "            if \"applied\" in easy_apply_button.text.strip().lower():\n",
    "                print(\"‚ÑπÔ∏è Already applied. Skipping this job.\")\n",
    "                return \"Already applied\"\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", easy_apply_button)\n",
    "            time.sleep(1)\n",
    "            driver.execute_script(\"arguments[0].click();\", easy_apply_button)\n",
    "            print(\"‚úÖ Easy Apply clicked!\")\n",
    "            return \"Clicked Easy Apply\"\n",
    "        except:\n",
    "            print(\"‚ÑπÔ∏è Easy Apply button not found or job not available. Skipping.\")\n",
    "            return \"Already applied\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # Extract Easy Apply fields\n",
    "    # -----------------------------\n",
    "    def extract_easy_apply_fields():\n",
    "        fields = []\n",
    "        try:\n",
    "            form_container = wait.until(\n",
    "                EC.visibility_of_element_located((By.CSS_SELECTOR, \"div.jobs-easy-apply-modal\"))\n",
    "            )\n",
    "\n",
    "            for inp in form_container.find_elements(By.TAG_NAME, \"input\"):\n",
    "                try:\n",
    "                    label_el = inp.find_element(By.XPATH, \"ancestor::div[@data-test-single-line-text-form-component]//label\")\n",
    "                    question_text = label_el.text.strip() if label_el else inp.get_attribute(\"aria-label\") or inp.get_attribute(\"id\")\n",
    "                    field_type = inp.get_attribute(\"type\")\n",
    "                    value = inp.get_attribute(\"value\") if field_type not in [\"checkbox\", \"radio\", \"file\"] else inp.is_selected()\n",
    "                    options = []\n",
    "\n",
    "                    if field_type in [\"radio\", \"checkbox\"]:\n",
    "                        name_attr = inp.get_attribute(\"name\")\n",
    "                        group = form_container.find_elements(By.NAME, name_attr)\n",
    "                        options = [el.get_attribute(\"aria-label\") for el in group if el.get_attribute(\"aria-label\")]\n",
    "\n",
    "                    fields.append({\n",
    "                        \"field_name\": question_text,\n",
    "                        \"field_type\": field_type,\n",
    "                        \"value\": value,\n",
    "                        \"options\": options,\n",
    "                        \"element\": inp\n",
    "                    })\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            for ta in form_container.find_elements(By.TAG_NAME, \"textarea\"):\n",
    "                try:\n",
    "                    label_el = ta.find_element(By.XPATH, \"ancestor::div[@data-test-single-line-text-form-component]//label\")\n",
    "                    question_text = label_el.text.strip() if label_el else ta.get_attribute(\"aria-label\") or ta.get_attribute(\"id\")\n",
    "                    fields.append({\n",
    "                        \"field_name\": question_text,\n",
    "                        \"field_type\": \"textarea\",\n",
    "                        \"value\": ta.get_attribute(\"value\"),\n",
    "                        \"options\": [],\n",
    "                        \"element\": ta\n",
    "                    })\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            for sel in form_container.find_elements(By.TAG_NAME, \"select\"):\n",
    "                try:\n",
    "                    label_el = sel.find_element(By.XPATH, \"ancestor::div[@data-test-form-element]//label\")\n",
    "                    question_text = label_el.text.strip() if label_el else sel.get_attribute(\"aria-label\") or sel.get_attribute(\"id\")\n",
    "                    options = [opt.text for opt in sel.find_elements(By.TAG_NAME, \"option\")]\n",
    "                    fields.append({\n",
    "                        \"field_name\": question_text,\n",
    "                        \"field_type\": \"select\",\n",
    "                        \"value\": sel.get_attribute(\"value\"),\n",
    "                        \"options\": options,\n",
    "                        \"element\": sel\n",
    "                    })\n",
    "                except:\n",
    "                    continue\n",
    "        except:\n",
    "            pass\n",
    "        return fields\n",
    "\n",
    "    # -----------------------------\n",
    "    # Fill Easy Apply fields\n",
    "    # -----------------------------\n",
    "    def fill_easy_apply_fields(fields):\n",
    "        for field in fields:\n",
    "            try:\n",
    "                el = field.get(\"element\")\n",
    "                if not el:\n",
    "                    continue\n",
    "                generated = field.get(\"generated_answer\", \"\").strip()\n",
    "                f_type = field[\"field_type\"]\n",
    "\n",
    "                if f_type in [\"text\", \"textarea\"] or el.get_attribute(\"contenteditable\") == \"true\":\n",
    "                    if generated:\n",
    "                        driver.execute_script(\"\"\"\n",
    "                            arguments[0].focus();\n",
    "                            arguments[0].value = arguments[1];\n",
    "                            arguments[0].dispatchEvent(new Event('input', { bubbles: true }));\n",
    "                            arguments[0].dispatchEvent(new Event('change', { bubbles: true }));\n",
    "                        \"\"\", el, generated)\n",
    "                        time.sleep(0.2)\n",
    "\n",
    "                elif f_type == \"select\" and generated:\n",
    "                    try:\n",
    "                        Select(el).select_by_visible_text(generated)\n",
    "                    except:\n",
    "                        el.click()\n",
    "                        time.sleep(0.3)\n",
    "                        option = el.find_element(By.XPATH, f\".//li[normalize-space(text())='{generated}']\")\n",
    "                        option.click()\n",
    "                        time.sleep(0.2)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # -----------------------------\n",
    "    # Dummy answer generator\n",
    "    # -----------------------------\n",
    "    def generate_field_answers(fields, resume_json):\n",
    "        for field in fields:\n",
    "            field[\"generated_answer\"] = \"Sample answer\"\n",
    "        return fields\n",
    "\n",
    "    # -----------------------------\n",
    "    # Login flow\n",
    "    # -----------------------------\n",
    "    if not try_login_with_cookies():\n",
    "        login_with_credentials()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Load resume + CSV\n",
    "    # -----------------------------\n",
    "    with open(resume_json_path, \"r\") as f:\n",
    "        resume_data = json.load(f)\n",
    "\n",
    "    with open(CSV_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        jobs = [row for row in reader if row[\"apply_type\"].lower() == \"easy apply\"]\n",
    "\n",
    "    print(f\"\\nüöÄ Starting auto-apply for {len(jobs)} Easy Apply jobs...\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Main job loop\n",
    "    # -----------------------------\n",
    "    for idx, job in enumerate(jobs, 1):\n",
    "        print(f\"\\nüéØ Processing job {idx}/{len(jobs)}: {job['title']} at {job['company']} ({job['location']})\")\n",
    "        driver.get(job[\"apply_link\"])\n",
    "        time.sleep(3)\n",
    "\n",
    "        apply_status = click_easy_apply()\n",
    "        if apply_status == \"Already applied\":\n",
    "            continue\n",
    "\n",
    "        step = 1\n",
    "        while True:\n",
    "            print(f\"‚û°Ô∏è Step {step}...\")\n",
    "            fields = extract_easy_apply_fields()\n",
    "\n",
    "            if not fields:\n",
    "                print(\"‚ÑπÔ∏è No fields detected. Checking for resume upload or next step...\")\n",
    "\n",
    "                try:\n",
    "                    upload_input = driver.find_element(By.CSS_SELECTOR, \"input[type='file'].hidden\")\n",
    "                    if upload_input.is_displayed() or \"hidden\" in upload_input.get_attribute(\"class\"):\n",
    "                        print(\"üìé Found resume upload input, uploading resume...\")\n",
    "                        local_resume = download_resume_from_drive(RESUME_DRIVE_URL)\n",
    "                        upload_input.send_keys(local_resume)\n",
    "                        time.sleep(2)\n",
    "                        os.remove(local_resume)\n",
    "                        print(\"‚úÖ Resume uploaded successfully.\")\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    next_button = driver.find_element(\n",
    "                        By.XPATH,\n",
    "                        \"//button[contains(., 'Next') or contains(., 'Continue') or contains(., 'Review')]\"\n",
    "                    )\n",
    "                    if next_button and next_button.is_enabled():\n",
    "                        driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "                        time.sleep(0.5)\n",
    "                        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                        print(\"‚û°Ô∏è Proceeded to next step.\")\n",
    "                        time.sleep(2)\n",
    "                        step += 1\n",
    "                        continue\n",
    "                except:\n",
    "                    print(\"‚ö†Ô∏è No next button found. Assuming final review.\")\n",
    "                    break\n",
    "\n",
    "                break\n",
    "\n",
    "            fields_with_answers = generate_field_answers(fields, resume_data)\n",
    "            fill_easy_apply_fields(fields_with_answers)\n",
    "\n",
    "            try:\n",
    "                next_button = wait.until(\n",
    "                    EC.presence_of_element_located(\n",
    "                        (By.XPATH, \"//button[contains(., 'Next') or contains(., 'Review') or contains(., 'Continue')]\")\n",
    "                    )\n",
    "                )\n",
    "                if \"disabled\" in next_button.get_attribute(\"class\"):\n",
    "                    break\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "                time.sleep(0.5)\n",
    "                driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                time.sleep(2)\n",
    "                step += 1\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        try:\n",
    "            submit_button = wait.until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (By.XPATH, \"//button[contains(., 'Submit') or contains(., 'Done')]\")\n",
    "                )\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", submit_button)\n",
    "            time.sleep(0.5)\n",
    "            driver.execute_script(\"arguments[0].click();\", submit_button)\n",
    "            print(\"‚úÖ Application submitted successfully!\")\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è Could not find Submit button, skipped.\")\n",
    "\n",
    "    driver.quit()\n",
    "    print(\"\\nüéâ All Easy Apply jobs processed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd168586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-28 14:32:32,839 - INFO - üß† Name candidate (largest font): Yeswanth Yerra\n",
      "2025-10-28 14:32:32,848 - INFO -  Average font size: 9.63\n",
      "2025-10-28 14:32:32,856 - INFO - ‚úÖ Extracted structured resume saved to: resumes/Yeswanth_Yerra_CV_structured.json\n",
      "2025-10-28 14:32:32,857 - INFO - ====== WebDriver manager ======\n",
      "2025-10-28 14:32:32,909 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-10-28 14:32:32,978 - INFO - Get LATEST chromedriver version for google-chrome\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Extracting structured resume data...\n",
      "‚úÖ Resume extraction complete ‚Üí resumes/Yeswanth_Yerra_CV_structured.json\n",
      "\n",
      "üåç Starting Indeed scraping...\n",
      "üîç Scraping Indeed for 'Machine Learning Engineer' jobs in 'India', Country: India\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-28 14:32:33,047 - INFO - Driver [/home/acer/.wdm/drivers/chromedriver/linux64/141.0.7390.122/chromedriver-linux64/chromedriver] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEBUG: Processing page 1 | URL: https://in.indeed.com/jobs?q=Machine+Learning+Engineer&l=India&from=searchOnDesktopSerp&start=0\n",
      "DEBUG: Found 16 job cards on page 1\n",
      "\n",
      "DEBUG: Processing job card #1 on page 1\n",
      "\n",
      "DEBUG: Processing job card #2 on page 1\n",
      "\n",
      "DEBUG: Processing job card #3 on page 1\n",
      "\n",
      "DEBUG: Processing job card #4 on page 1\n",
      "\n",
      "DEBUG: Processing job card #5 on page 1\n",
      "\n",
      "DEBUG: Processing job card #6 on page 1\n",
      "\n",
      "DEBUG: Processing job card #7 on page 1\n",
      "\n",
      "DEBUG: Processing job card #8 on page 1\n",
      "\n",
      "DEBUG: Processing job card #9 on page 1\n",
      "\n",
      "DEBUG: Processing job card #10 on page 1\n",
      "\n",
      "DEBUG: Processing job card #11 on page 1\n",
      "\n",
      "DEBUG: Processing job card #12 on page 1\n",
      "\n",
      "DEBUG: Processing job card #13 on page 1\n",
      "\n",
      "DEBUG: Processing job card #14 on page 1\n",
      "\n",
      "DEBUG: Processing job card #15 on page 1\n",
      "\n",
      "DEBUG: Processing job card #16 on page 1\n",
      "\n",
      "DEBUG: Processing page 2 | URL: https://in.indeed.com/jobs?q=Machine+Learning+Engineer&l=India&from=searchOnDesktopSerp&start=10\n",
      "DEBUG: Found 16 job cards on page 2\n",
      "\n",
      "DEBUG: Processing job card #1 on page 2\n",
      "\n",
      "DEBUG: Processing job card #2 on page 2\n",
      "\n",
      "DEBUG: Processing job card #3 on page 2\n",
      "\n",
      "DEBUG: Processing job card #4 on page 2\n",
      "\n",
      "DEBUG: Processing job card #5 on page 2\n",
      "\n",
      "DEBUG: Processing job card #6 on page 2\n",
      "\n",
      "DEBUG: Processing job card #7 on page 2\n",
      "\n",
      "DEBUG: Processing job card #8 on page 2\n",
      "\n",
      "DEBUG: Processing job card #9 on page 2\n",
      "\n",
      "DEBUG: Processing job card #10 on page 2\n",
      "\n",
      "DEBUG: Processing job card #11 on page 2\n",
      "\n",
      "DEBUG: Processing job card #12 on page 2\n",
      "\n",
      "DEBUG: Processing job card #13 on page 2\n",
      "\n",
      "DEBUG: Processing job card #14 on page 2\n",
      "\n",
      "DEBUG: Processing job card #15 on page 2\n",
      "\n",
      "DEBUG: Processing job card #16 on page 2\n",
      "\n",
      "‚úÖ Scraping complete. Saved to csv/indeed_jobs.csv\n",
      "‚úÖ Indeed scraping complete ‚Üí csv/indeed_jobs.csv\n",
      "\n",
      "üíº Starting LinkedIn scraping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-28 14:33:04,116 - INFO - Chrome WebDriver initialized successfully\n",
      "2025-10-28 14:33:09,742 - INFO - Scraping 'Machine Learning Engineer' jobs in India (pages=1)\n",
      "2025-10-28 14:33:17,839 - INFO - Found 70 job cards\n",
      "2025-10-28 14:38:12,761 - INFO - Saved 70 jobs to csv/linkedin_jobs.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LinkedIn scraping complete ‚Üí csv/linkedin_jobs.csv\n",
      "\n",
      "ü§ñ Starting LinkedIn auto-apply (Easy Apply only)...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'apply_easy_apply_jobs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 51\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# ü§ñ 4. Auto-Apply on LinkedIn (Easy Apply)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mü§ñ Starting LinkedIn auto-apply (Easy Apply only)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m \u001b[43mapply_easy_apply_jobs\u001b[49m(\n\u001b[1;32m     52\u001b[0m     csv_path\u001b[38;5;241m=\u001b[39mlinkedin_csv_path,\n\u001b[1;32m     53\u001b[0m     resume_json_path\u001b[38;5;241m=\u001b[39mstructured_json_path,\n\u001b[1;32m     54\u001b[0m     resume_drive_url\u001b[38;5;241m=\u001b[39mresume_drive_url\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müéØ Pipeline completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'apply_easy_apply_jobs' is not defined"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# üöÄ Full AI Career Pipeline Test\n",
    "# ===============================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# --- File Paths ---\n",
    "pdf_resume_path = \"resumes/Yeswanth_Yerra_CV.pdf\"\n",
    "structured_json_path = pdf_resume_path.replace(\".pdf\", \"_structured.json\")\n",
    "linkedin_csv_path = \"csv/linkedin_jobs.csv\"\n",
    "indeed_csv_path = \"csv/indeed_jobs.csv\"\n",
    "\n",
    "# --- Google Drive resume URL (for Easy Apply upload) ---\n",
    "resume_drive_url = \"https://drive.google.com/file/d/1ZCnnE0SHsyPqZpZBbnWz5npLppl7UZXh/view?usp=drive_link\"\n",
    "\n",
    "# ===============================\n",
    "# üß© 1. Resume Extraction\n",
    "# ===============================\n",
    "print(\"\\nüß† Extracting structured resume data...\")\n",
    "resume_data = extract_resume_data(pdf_resume_path)\n",
    "resume_data = split_multiline_sections(resume_data)\n",
    "save_to_json(pdf_resume_path, resume_data)\n",
    "print(f\"‚úÖ Resume extraction complete ‚Üí {structured_json_path}\")\n",
    "\n",
    "# ===============================\n",
    "# üåç 2. Indeed Job Scraping\n",
    "# ===============================\n",
    "print(\"\\nüåç Starting Indeed scraping...\")\n",
    "indeed_jobs = scrape_indeed_jobs(max_pages=2, save_csv=True, output_file=indeed_csv_path)\n",
    "print(f\"‚úÖ Indeed scraping complete ‚Üí {indeed_csv_path}\")\n",
    "\n",
    "# ===============================\n",
    "# üíº 3. LinkedIn Job Scraping\n",
    "# ===============================\n",
    "print(\"\\nüíº Starting LinkedIn scraping...\")\n",
    "linkedin_scraper = LinkedInJobScraper(headless=True)\n",
    "preferred_title, preferred_location = get_preferred_job_and_location(structured_json_path)\n",
    "linkedin_jobs = linkedin_scraper.scrape_jobs(\n",
    "    job_title=preferred_title,\n",
    "    location=preferred_location,\n",
    "    pages=1,\n",
    "    days_back=7,\n",
    "    csv_filename=linkedin_csv_path\n",
    ")\n",
    "print(f\"‚úÖ LinkedIn scraping complete ‚Üí {linkedin_csv_path}\")\n",
    "\n",
    "# ===============================\n",
    "# ü§ñ 4. Auto-Apply on LinkedIn (Easy Apply)\n",
    "# ===============================\n",
    "print(\"\\nü§ñ Starting LinkedIn auto-apply (Easy Apply only)...\")\n",
    "apply_easy_apply_jobs(\n",
    "    csv_path=linkedin_csv_path,\n",
    "    resume_json_path=structured_json_path,\n",
    "    resume_drive_url=resume_drive_url\n",
    ")\n",
    "print(\"\\nüéØ Pipeline completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
